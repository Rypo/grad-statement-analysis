{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Phrase Extraction and Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook attempts to synthesize useful information from the reviews using key phrase extraction and summarization.\n",
    "\n",
    "The task is accomplished with two specialized packages, [pke](https://github.com/boudinfl/pke) for phrase extraction and [sumy](https://github.com/miso-belica/sumy) for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rygu\\Anaconda3\\envs\\scrape\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import inspect\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "import pke\n",
    "from pke.unsupervised import FirstPhrases,KPMiner,TfIdf,YAKE\n",
    "from pke.unsupervised import MultipartiteRank,PositionRank,SingleRank,TextRank,TopicalPageRank,TopicRank\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers import kl, lex_rank, luhn, text_rank, lsa, reduction\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdmn\n",
    "import logging\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = Path('proc_data/raw_texts/critiques/')\n",
    "cpath = Path('proc_data/raw_texts/crits_dedup')\n",
    "docs = [*cpath.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Google stopwords list from the 2000's \n",
    "# from: https://www.ranks.nl/stopwords\n",
    "# plus a few custom add-ins\n",
    "minimal_stops = set(\n",
    "    ['i','a','about','an','are','as','at','be','by','for',\n",
    "     'from','how','in','is','it','of','on','or','that','the','this',\n",
    "     'to','was','what','when','where','who','will','with'] # -com, -www\n",
    "    + ['and','your','essay','have','so','my'] \n",
    "    + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def to_file_corpus(df, datapath, idcol='doc_id_rv', postcol='post_content_rv'):\n",
    "    print(f'Writting {df.shape[0]} files to: {datapath} ...')\n",
    "    for doc_id, content in df[[idcol,postcol]].values:\n",
    "        Path(datapath/doc_id).with_suffix('.txt').write_text(content)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge= pd.read_feather('proc_data/merge_deduped.df')\n",
    "df_merge = df_merge.applymap(unidecode)\n",
    "rmcrits = df_merge.post_content_rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a term frequency file for use in `KPMiner` and `Tfidf`.\n",
    "\n",
    "Note: these models consistently underperformed other methods, so ultimately, this file was not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df_text = pd.read_pickle('proc_data/minparse/full_text_minproc.pkl')\n",
    "crit_idx = np.load('proc_data/minparse/crit_idx.npy')\n",
    "df_crit = df_text.loc[crit_idx]\n",
    "crit_content = df_crit['post_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "vec_params = dict(strip_accents='ascii', ngram_range=(1,5), max_df=0.8, min_df=5, stop_words=list(minimal_stops))\n",
    "count_vctzr = CountVectorizer(**vec_params)\n",
    "\n",
    "count_mat_crit = count_vctzr.fit_transform(crit_content)\n",
    "feat_names = count_vctzr.get_feature_names()\n",
    "\n",
    "term_counts = pd.Series(count_mat_crit.sum(axis=0).A1, index=feat_names)\n",
    "term_counts.to_csv('proc_data/raw_texts/crit_termcounts.tsv',sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\t6\n",
      "000\t19\n",
      "10\t106\n",
      "10 year\t5\n",
      "10 years\t25\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 'proc_data/raw_texts/crit_termcounts.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!sed -i '1i--NB_DOC--\\t{count_mat_crit.shape[0]}' proc_data/raw_texts/crit_termcounts.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--NB_DOC--\t5534\n",
      "00\t6\n",
      "000\t19\n",
      "10\t106\n",
      "10 year\t5\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 'proc_data/raw_texts/crit_termcounts.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrcrs = [SingleRank(), TextRank(), PositionRank(), TopicalPageRank(), MultipartiteRank(), TopicRank(), YAKE()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SingleRank =====\n",
      "select: ['pos']\n",
      "weight: ['window', 'pos', 'normalized']\n",
      "===== TextRank =====\n",
      "select: ['pos']\n",
      "weight: ['window', 'pos', 'top_percent', 'normalized']\n",
      "===== PositionRank =====\n",
      "select: ['grammar', 'maximum_word_number', 'kwargs']\n",
      "weight: ['window', 'pos', 'normalized']\n",
      "===== TopicalPageRank =====\n",
      "select: ['grammar', 'kwargs']\n",
      "weight: ['window', 'pos', 'lda_model', 'stoplist', 'normalized']\n",
      "===== MultipartiteRank =====\n",
      "select: ['pos', 'stoplist']\n",
      "weight: ['threshold', 'method', 'alpha']\n",
      "===== TopicRank =====\n",
      "select: ['pos', 'stoplist']\n",
      "weight: ['threshold', 'method', 'heuristic']\n",
      "===== YAKE =====\n",
      "select: ['n', 'stoplist', 'kwargs']\n",
      "weight: ['window', 'stoplist', 'use_stems']\n"
     ]
    }
   ],
   "source": [
    "for x in xtrcrs:\n",
    "    print(f\"===== {str(x.__class__).split('.')[-1][:-2]} =====\")\n",
    "    print('select:',[*inspect.signature(x.candidate_selection).parameters.keys()])\n",
    "    print('weight:',[*inspect.signature(x.candidate_weighting).parameters.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://boudinfl.github.io/pke/build/html/unsupervised.html\n",
    "logging.disable(level=logging.CRITICAL)\n",
    "def kpextract(extractor, doc, pos=None, topn=10, show=False, termfreq_file=None):\n",
    "    \"\"\"Apply multiple keyphrase extraction methods to a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    extractor : pke.unsupervised Model\n",
    "        Keyphrase extraction model to use.\n",
    "    doc : str or filepath-object\n",
    "        Text passage to apply keyphrase extraction\n",
    "    pos : list[str], (default: ['NOUN','VERB','ADJ','ADV'])\n",
    "        Part of speech tags used in canidate selection and weighting when applicable.\n",
    "    topn : int, (default: 10)\n",
    "        Number of phrases to extract for each method.\n",
    "    show : bool, (default: False)\n",
    "        If true, print results as they are calculated\n",
    "    termfreq_file : str or filepath-object, (default: None)\n",
    "        Path to term frequency document. Only required when 'kpminer' or 'tfidf' are included\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    phrases : list[tuple(extractor:str, phrase:str, score:float)]]\n",
    "        a list of each extracted phrase containing\n",
    "        tuples of the extraction method, extracted phrase, and score of the phrase\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.as_posix() if isinstance(doc, Path) else doc\n",
    "    pos = pos if pos is not None else set(['NOUN','VERB','ADJ','ADV'])\n",
    "    phrases = []\n",
    "\n",
    "    name = str(extractor.__class__).split('.')[-1][:-2]\n",
    "    if show: print(f\"===== {name} =====\")\n",
    "    \n",
    "    if name not in ['KPMiner','TfIdf']:\n",
    "        extractor.load_document(input=doc, language='en', encoding='utf-8')\n",
    "        extractor.candidate_selection(pos=pos)\n",
    "        try:\n",
    "            if name in ['SingleRank', 'TextRank', 'PositionRank']: \n",
    "                extractor.candidate_weighting(pos=pos)\n",
    "            else: \n",
    "                extractor.candidate_weighting()\n",
    "        except Exception as e: # ignore TPR exception when it fails to product candidates \n",
    "            if show: print(e)\n",
    "    else:\n",
    "        assert termfreq_file is not None, \"Term frequency file required for KPMiner and TfIdf: pass a valid filepath to `termfreq_file`\"\n",
    "        docfreq = pke.load_document_frequency_file(input_file=termcount_file)\n",
    "        extractor.load_document(input=doc, language='en')\n",
    "\n",
    "        extractor.candidate_selection(lasf=3, cutoff=200, n=5, stoplist=list(minimal_stops))\n",
    "        extractor.candidate_weighting(df=docfreq)\n",
    "\n",
    "    keyphrases = extractor.get_n_best(n=topn)\n",
    "    for x,y in keyphrases:\n",
    "        phrases.append((name,x,y))\n",
    "        if show: print(f'{y:<10.4f} {x}')\n",
    "            \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunkanmi, did the university that you are applying to provide you with a prompt for your personal statement? If they did not, then you still have a chance to correct the content of your essay. This essay just doesn't fall within the expected requirements of a masters degree personal statement. It sounds more like you drafted this for a college application instead. Therefore, you need to work on creating a more relevant personal statement for your application.\n",
      "\n",
      "For starters, the focus of the essay should consider writing a shorter, more informative and relevant essay. A reviewer will normally offer 5 minutes to reading your essay before he decides it has taken too long to get to the point then sets it aside for future reading. Try to keep this essay short because this is a statement for graduate school. So the information you indicate should represent more of your college academics and professional experience. Even though your mother was the reason that you started your medical career, from what I have read, there are other, non-related reasons that you have opted to pursue Public Health. Suppress the urge to concentrate on the discussion of your mother's death. If you must, mention it towards the end of your essay. Don't place it at the beginning. That is the act of a college applicant, not a masters degree student.\n",
      "\n",
      "Keep the focus on you as a person. The clue as to what to write is in the name of the essay itself. A personal essay. So talk more about yourself and how you developed this interest. Right now, your essay is a cross between a statement and a research paper. don't lose focus of the keyword. Just so you can redirect the essay content.\n",
      "\n",
      "Also, show the reviewer that you know about the program you are getting into. Show a sense of familiarity with the expectations of the course and explain how your personality will thrive in this environment. What interested you in their school? Is there a professor you wish to study with or something? You must find a way to indicate that everything in your life led to this very moment. Make your vision for your career clear.\n"
     ]
    }
   ],
   "source": [
    "samp = rmcrits.sample(1)\n",
    "print(samp.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SingleRank =====\n",
      "0.0755     essay just does n't fall\n",
      "0.0736     more relevant personal statement\n",
      "0.0665     essay should consider writing\n",
      "0.0653     personal essay\n",
      "0.0628     essay is\n",
      "0.0599     masters degree personal statement\n",
      "0.0535     relevant essay\n",
      "0.0533     essay content\n",
      "0.0493     indicate should represent more\n",
      "0.0489     essay short\n",
      "===== TextRank =====\n",
      "0.0821     essay just does n't fall\n",
      "0.0663     essay should consider writing\n",
      "0.0642     more relevant personal statement\n",
      "0.0559     indicate should represent more\n",
      "0.0521     personal essay\n",
      "0.0511     do n't lose focus\n",
      "0.0496     masters degree personal statement\n",
      "0.0488     personality will thrive\n",
      "0.0445     reviewer will normally offer\n",
      "0.0432     non - related reasons\n",
      "===== PositionRank =====\n",
      "0.0792     personal essay\n",
      "0.0740     relevant personal statement\n",
      "0.0640     personal statement\n",
      "0.0571     essay content\n",
      "0.0558     relevant essay\n",
      "0.0458     essay\n",
      "0.0335     person\n",
      "0.0305     statement\n",
      "0.0259     college application\n",
      "0.0242     masters degree student\n",
      "===== TopicRank =====\n",
      "0.0524     essay\n",
      "0.0337     personal statement\n",
      "0.0281     college application instead\n",
      "0.0244     must\n",
      "0.0238     reading\n",
      "0.0238     show\n",
      "0.0234     keep\n",
      "0.0207     graduate school\n",
      "0.0199     content\n",
      "0.0195     focus\n",
      "===== MultipartiteRank =====\n",
      "0.0527     essay\n",
      "0.0367     personal statement\n",
      "0.0283     college application instead\n",
      "0.0221     reading\n",
      "0.0202     content\n",
      "0.0195     keep\n",
      "0.0188     must\n",
      "0.0176     focus\n",
      "0.0172     expected requirements\n",
      "0.0161     statement\n",
      "===== TopicalPageRank =====\n",
      "0.1035     personal essay\n",
      "0.0900     relevant personal statement\n",
      "0.0860     relevant essay\n",
      "0.0810     essay content\n",
      "0.0735     personal statement\n",
      "0.0694     essay\n",
      "0.0646     non - related reasons\n",
      "0.0449     masters degree student\n",
      "0.0448     college application\n",
      "0.0394     statement\n",
      "===== YAKE =====\n",
      "0.0317     essay\n",
      "0.0420     personal statement\n",
      "0.0668     statement\n",
      "0.0838     personal\n",
      "0.0854     relevant personal statement\n",
      "0.0925     degree personal statement\n",
      "0.1324     personal essay\n",
      "0.1593     college\n",
      "0.1661     sunkanmi\n",
      "0.1693     focus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('SingleRank', \"essay just does n't fall\", 0.07550260655124301),\n",
       "  ('SingleRank', 'more relevant personal statement', 0.07358865299907494),\n",
       "  ('SingleRank', 'essay should consider writing', 0.06653609641263179),\n",
       "  ('SingleRank', 'personal essay', 0.06530097046880788),\n",
       "  ('SingleRank', 'essay is', 0.06280347902841764),\n",
       "  ('SingleRank', 'masters degree personal statement', 0.059891787799678735),\n",
       "  ('SingleRank', 'relevant essay', 0.05350622811292505),\n",
       "  ('SingleRank', 'essay content', 0.0533159836964467),\n",
       "  ('SingleRank', 'indicate should represent more', 0.04933620153832671),\n",
       "  ('SingleRank', 'essay short', 0.04885619372052423)],\n",
       " [('TextRank', \"essay just does n't fall\", 0.08212028743866907),\n",
       "  ('TextRank', 'essay should consider writing', 0.06631508028225536),\n",
       "  ('TextRank', 'more relevant personal statement', 0.06415624119462564),\n",
       "  ('TextRank', 'indicate should represent more', 0.055906057154897),\n",
       "  ('TextRank', 'personal essay', 0.05212557413230238),\n",
       "  ('TextRank', \"do n't lose focus\", 0.051135163431518214),\n",
       "  ('TextRank', 'masters degree personal statement', 0.049618259013396754),\n",
       "  ('TextRank', 'personality will thrive', 0.04877958244617698),\n",
       "  ('TextRank', 'reviewer will normally offer', 0.04446492171714017),\n",
       "  ('TextRank', 'non - related reasons', 0.043245170341887254)],\n",
       " [('PositionRank', 'personal essay', 0.07924186111978622),\n",
       "  ('PositionRank', 'relevant personal statement', 0.07401366050241887),\n",
       "  ('PositionRank', 'personal statement', 0.06395527500968246),\n",
       "  ('PositionRank', 'essay content', 0.05712897208070583),\n",
       "  ('PositionRank', 'relevant essay', 0.05583550314587431),\n",
       "  ('PositionRank', 'essay', 0.045777117653137904),\n",
       "  ('PositionRank', 'person', 0.033464743466648315),\n",
       "  ('PositionRank', 'statement', 0.03049053154303414),\n",
       "  ('PositionRank', 'college application', 0.0259465751011075),\n",
       "  ('PositionRank', 'masters degree student', 0.024216496702037803)],\n",
       " [('TopicRank', 'essay', 0.05237136790073719),\n",
       "  ('TopicRank', 'personal statement', 0.033655979778452104),\n",
       "  ('TopicRank', 'college application instead', 0.028097926219740322),\n",
       "  ('TopicRank', 'must', 0.024367312766957673),\n",
       "  ('TopicRank', 'reading', 0.023816176822367054),\n",
       "  ('TopicRank', 'show', 0.023784201181911987),\n",
       "  ('TopicRank', 'keep', 0.023420357546518267),\n",
       "  ('TopicRank', 'graduate school', 0.020654184828811546),\n",
       "  ('TopicRank', 'content', 0.019891661847497556),\n",
       "  ('TopicRank', 'focus', 0.01945591497332461)],\n",
       " [('MultipartiteRank', 'essay', 0.052693746368620925),\n",
       "  ('MultipartiteRank', 'personal statement', 0.036674283506953856),\n",
       "  ('MultipartiteRank', 'college application instead', 0.028321140913585452),\n",
       "  ('MultipartiteRank', 'reading', 0.022068342417117696),\n",
       "  ('MultipartiteRank', 'content', 0.020155170538695987),\n",
       "  ('MultipartiteRank', 'keep', 0.019511374219887058),\n",
       "  ('MultipartiteRank', 'must', 0.018821287109012312),\n",
       "  ('MultipartiteRank', 'focus', 0.0175962452581583),\n",
       "  ('MultipartiteRank', 'expected requirements', 0.017233740653298957),\n",
       "  ('MultipartiteRank', 'statement', 0.016091837003434677)],\n",
       " [('TopicalPageRank', 'personal essay', 0.10345087285437178),\n",
       "  ('TopicalPageRank', 'relevant personal statement', 0.09004116344004595),\n",
       "  ('TopicalPageRank', 'relevant essay', 0.08601108790684626),\n",
       "  ('TopicalPageRank', 'essay content', 0.08098358949771109),\n",
       "  ('TopicalPageRank', 'personal statement', 0.0734584260467166),\n",
       "  ('TopicalPageRank', 'essay', 0.06942835051351691),\n",
       "  ('TopicalPageRank', 'non - related reasons', 0.06457998218932902),\n",
       "  ('TopicalPageRank', 'masters degree student', 0.0449089886342328),\n",
       "  ('TopicalPageRank', 'college application', 0.0447882389470612),\n",
       "  ('TopicalPageRank', 'statement', 0.03943590370586173)],\n",
       " [('YAKE', 'essay', 0.03169125465292965),\n",
       "  ('YAKE', 'personal statement', 0.04202759554232961),\n",
       "  ('YAKE', 'statement', 0.0667797287425911),\n",
       "  ('YAKE', 'personal', 0.08383219405867884),\n",
       "  ('YAKE', 'relevant personal statement', 0.08537674829442557),\n",
       "  ('YAKE', 'degree personal statement', 0.09250806677900521),\n",
       "  ('YAKE', 'personal essay', 0.13237059658546327),\n",
       "  ('YAKE', 'college', 0.1592674165248625),\n",
       "  ('YAKE', 'sunkanmi', 0.16608787239325115),\n",
       "  ('YAKE', 'focus', 0.16928512932358786)]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_extract(samp.iloc[0], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SingleRank =====\n",
      "===== TextRank =====\n",
      "===== PositionRank =====\n",
      "0.0000     sunkanmi\n",
      "0.0000     university\n",
      "0.0000     prompt\n",
      "0.0000     personal statement\n",
      "0.0000     chance\n",
      "0.0000     content\n",
      "0.0000     essay\n",
      "0.0000     requirements\n",
      "0.0000     masters degree\n",
      "0.0000     college application\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [('PositionRank', 'sunkanmi', 0.0),\n",
       "  ('PositionRank', 'university', 0.0),\n",
       "  ('PositionRank', 'prompt', 0.0),\n",
       "  ('PositionRank', 'personal statement', 0.0),\n",
       "  ('PositionRank', 'chance', 0.0),\n",
       "  ('PositionRank', 'content', 0.0),\n",
       "  ('PositionRank', 'essay', 0.0),\n",
       "  ('PositionRank', 'requirements', 0.0),\n",
       "  ('PositionRank', 'masters degree', 0.0),\n",
       "  ('PositionRank', 'college application', 0.0)]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_extract(samp.iloc[0], include=['single','text','position'], pos=['VBZ'], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_extract(doc, include=None, exclude=None, pos=None, topn=10, show=False, termfreq_file=None):\n",
    "    \"\"\"Apply multiple keyphrase extraction methods to a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str or filepath-object\n",
    "        Text passage to apply keyphrase extraction\n",
    "    include : list[str], (default: ['single','text','position','topic','multipartite','topical','yake'])\n",
    "        Extraction methods to use. Valid options: \n",
    "        ['single','text','position','topic','multipartite','topical','yake','kpminer','tfidf','firstphrases']\n",
    "    exclude : list[str], (default: None)\n",
    "        Extraction methods to omit. If not None, all valid, non-excluded options will be used.\n",
    "    pos : list[str], (default: None)\n",
    "        Part of speech tags. Passed to `kpextract`\n",
    "    topn : int, (default: 10)\n",
    "        Number of phrases to extract for each method\n",
    "    show : bool, (default: False)\n",
    "        If true, print results as they are calculated\n",
    "    termfreq_file : str or filepath-object, (default: None)\n",
    "        Path to term frequency document. Only required when 'kpminer' or 'tfidf' are included\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    phrases : list[list[tuple(extractor:str, phrase:str, score:float)]]\n",
    "        A list of each included extractor containing\n",
    "        a list of each extracted phrase containing\n",
    "        tuples of the extraction method, extracted phrase, and score of the phrase\n",
    "    \"\"\"\n",
    "    optnames = ['single','text','position','topic','multipartite','topical','yake',\n",
    "                'kpminer','tfidf','firstphrases']\n",
    "    opts = [SingleRank(), TextRank(), PositionRank(), TopicRank(), MultipartiteRank(), TopicalPageRank(), YAKE(),\n",
    "            KPMiner(), TfIdf(), FirstPhrases()]\n",
    "    \n",
    "    optdict = {k:v for k,v in zip(optnames, opts)}\n",
    "    default = optnames[:7]\n",
    "    \n",
    "    if include is None:\n",
    "        extnames = default if exclude is None else list(set(optnames)-set(exclude))\n",
    "    else:\n",
    "        extnames = include\n",
    "    \n",
    "    extractors = [optdict[x] for x in extnames]\n",
    "    \n",
    "    phrases = []\n",
    "    for extractor in extractors:\n",
    "        phrases.append(kpextract(extractor=extractor, doc=doc, pos=pos, topn=topn, show=show, termfreq_file=termfreq_file))\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def extract_all(doc, include_tpr=True, include_freq_models=False, termcount_file=None, show=False):\n",
    "    \"\"\"Deprecated. TODO: replace with `multi_extract`\"\"\"\n",
    "    st_extractors = [SingleRank(), TextRank()]\n",
    "    extractors = [PositionRank(), MultipartiteRank(), TopicRank(), YAKE()]  #FirstPhrases()\n",
    "    \n",
    "    docp = doc.as_posix() if isinstance(doc, Path) else doc\n",
    "    \n",
    "    pos=set(['NOUN','VERB','ADJ','ADV'])\n",
    "    phrases = []\n",
    "\n",
    "    for extractor in st_extractors:\n",
    "        name = str(extractor.__class__).split('.')[-1][:-2]\n",
    "        if show: print(f\"===== {name} =====\")\n",
    "        extractor.load_document(input=docp, language='en', encoding='utf-8')\n",
    "        extractor.candidate_selection(pos=pos)\n",
    "        extractor.candidate_weighting(window=10, pos=pos) # top_percent=0.33\n",
    "\n",
    "        keyphrases = extractor.get_n_best(n=10)\n",
    "        for x,y in keyphrases:\n",
    "            phrases.append((name,x,y))\n",
    "            if show: print(f'{y:<10.4f} {x}')\n",
    "\n",
    "    for extractor in extractors: # extractors\n",
    "        name = str(extractor.__class__).split('.')[-1][:-2]\n",
    "        if show: print(f\"===== {name} =====\")\n",
    "        extractor.load_document(input=docp, language='en', encoding='utf-8')\n",
    "        extractor.candidate_selection(pos=pos)\n",
    "        extractor.candidate_weighting() #pos=set(['NOUN','VERB','ADJ'])\n",
    "\n",
    "        keyphrases = extractor.get_n_best(n=10)\n",
    "        for x,y in keyphrases:\n",
    "            phrases.append((name,x,y))\n",
    "            if show: print(f'{y:<10.4f} {x}')\n",
    "\n",
    "    if include_tpr:\n",
    "        try:\n",
    "            extractor = TopicalPageRank()\n",
    "            name = str(extractor.__class__).split('.')[-1][:-2]\n",
    "            if show: print(f\"===== {name} =====\")\n",
    "            extractor.load_document(input=docp, language='en', encoding='utf-8')\n",
    "            extractor.candidate_selection(pos=pos)\n",
    "            extractor.candidate_weighting() #pos=set(['NOUN','VERB','ADJ'])\n",
    "\n",
    "            keyphrases = extractor.get_n_best(n=10)\n",
    "            for x,y in keyphrases:\n",
    "                phrases.append((name,x,y))\n",
    "                if show: print(f'{y:<10.4f} {x}')\n",
    "        except Exception as e: # ignore TPR exception when it fails to product candidates \n",
    "            if show: print(e)\n",
    "\n",
    "    if include_freq_models:\n",
    "        df_extractors = [KPMiner(), TfIdf()]\n",
    "        docfreq = pke.load_document_frequency_file(input_file=termcount_file \n",
    "                                                   if termcount_file is not None \n",
    "                                                   else 'proc_data/raw_texts/crit_termcounts.tsv')\n",
    "        for extractor in df_extractors:\n",
    "            name = str(extractor.__class__).split('.')[-1][:-2]\n",
    "            if show: print(f\"===== {name} =====\")\n",
    "            extractor.load_document(input=docp, language='en')\n",
    "\n",
    "            extractor.candidate_selection(lasf=3, cutoff=200, n=5, stoplist=list(minimal_stops))\n",
    "            extractor.candidate_weighting(df=docfreq) #sigma=3.0, alpha=2.3\n",
    "\n",
    "            keyphrases = extractor.get_n_best(n=10)\n",
    "            for x,y in keyphrases:\n",
    "                phrases.append((name,x,y))\n",
    "            if show: print(f'{y:<10.4f} {x}')\n",
    "\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proc_data\\raw_texts\\crits_dedup\\16657_10.txt\n",
      "\n",
      "Thanks for that discussion of the roles played by those health professionals, Casey.\n",
      "\n",
      "I think of that kind of profession within its historical context... like, in every culture, the healers have been multidisciplinary practitioners who did what they knew how to do and got the best results possible.\n",
      "\n",
      "So, all practitioners would be considered healers with some being more advanced than others. So, when I hear about any medical profession, I just thing \n",
      "\n",
      "In human society, a healer is a healer. And you'll gradually learn more modalities. Like, you might learn acupressure and trigger point work, for example,to compliment what you do. You might learn Ericksonian hypnosis. No matter what your day job is, you can be a part time freelance healer and try out various forms of therapy with various groups. It's good to get passionate about your own blend of therapeutic modalities.\n"
     ]
    }
   ],
   "source": [
    "exdoc = np.random.choice(docs)\n",
    "print(exdoc, exdoc.read_text(), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can choices command? I don't know if I like that intro... but you could try different verbs...\n",
      "\n",
      "There is A sense of clarity within me that intuitively makes me enables me to intuit a calling to Bi omedical Engineering.---here is an idea I had for you.\n",
      "\n",
      "I believe The central nervous system is the most\n",
      "\n",
      " All this content is ineffective. Anyone can make claims.\n",
      "\n",
      "This part is excellent, though, because it tells something about your unique experiences: ... feel confident for  about new academic challenges. Nice!I seemed to criticize you a lot, but actually the essay is very impressive. I wish you would include more specific examples of your intentions, your plans for how you might specialize, etc. This definitely shows great writing skill, too.\n"
     ]
    }
   ],
   "source": [
    "samp = rmcrits.sample(1)\n",
    "print(samp.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# disable warnings about default LDA model and fewer than 10 extracts\n",
    "logging.disable(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# skip reviews with less than 100 characters\n",
    "sm_rmcrits = rmcrits[rmcrits.str.len() > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ee51a5d2494a0aafcc80cb926e5587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_phrases = Parallel(n_jobs=3)(delayed(multi_extract)(x) for x in tqdmn(sm_rmcrits))\n",
    "pickle.dump(extracted_phrases,open('proc_data/exphrases_no_tpr.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072b49ebcf6d4ad0a8e220be60983882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tpr_extracted_phrases = Parallel(n_jobs=3)(delayed(multi_extract)(x) for x in tqdmn(sm_rmcrits))\n",
    "pickle.dump(tpr_extracted_phrases,open('proc_data/exphrases_tpr.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "extracted_phrases = pickle.load(open('proc_data/exphrases_no_tpr.pkl','rb'))\n",
    "tpr_extracted_phrases = pickle.load(open('proc_data/exphrases_tpr.pkl','rb'))\n",
    "extracted = [e + t for e,t in zip(extracted_phrases,tpr_extracted_phrases)]\n",
    "pickle.dump(extracted,open('proc_data/exphrases.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_docid = df_merge[df_merge.post_content_rv.str.len() > 100]['doc_id_rv']\n",
    "extracted = pickle.load(open('proc_data/exphrases.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = pd.DataFrame([*zip(sm_docid,extracted)],columns=['doc_id','rest']).explode('rest') # fill doc_ids\n",
    "exphrase_df = pd.DataFrame([(a,p,s) for ex in extracted for a,p,s in ex], columns=['kpe','phrase','score'])\n",
    "\n",
    "exphrase_df['doc_id'] = interm.reset_index()['doc_id']\n",
    "exphrase_df = exphrase_df.iloc[:, np.r_[-1,0:3]] # reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005076340349096021"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(exphrase_df[(exphrase_df['kpe'] == 'YAKE') & (exphrase_df['score'] > 1)].shape[0] \n",
    " / exphrase_df[(exphrase_df['kpe'] == 'YAKE')].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small percentage (~0.05%) of `YAKE` values fall outside of the 0-1 range. This few outliers does not warrant rescaling, these will be omitted. Similarly, `TopicalPageRank` values exceed 0-1 in instances where underscores are prevalent.\n",
    "\n",
    "`FirstPhrases` will also be omitted since it is purely a baseline test to assess relative model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>kpe</th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134661</th>\n",
       "      <td>63711_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>8.358186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134662</th>\n",
       "      <td>63711_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>7.010091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134663</th>\n",
       "      <td>63711_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>6.740472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52668</th>\n",
       "      <td>77173_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>like the name</td>\n",
       "      <td>5.584342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355310</th>\n",
       "      <td>12081_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>good idea</td>\n",
       "      <td>4.963447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222474</th>\n",
       "      <td>51326_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>paragraph only contains</td>\n",
       "      <td>4.719739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306283</th>\n",
       "      <td>7664_2</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>give the reasons</td>\n",
       "      <td>4.389942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85128</th>\n",
       "      <td>63636_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>3.890064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86121</th>\n",
       "      <td>54768_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>search in google</td>\n",
       "      <td>3.884757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85129</th>\n",
       "      <td>63636_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>3.787694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85130</th>\n",
       "      <td>63636_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>3.685324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85131</th>\n",
       "      <td>63636_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>3.582954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389080</th>\n",
       "      <td>16702_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>uni are pretty</td>\n",
       "      <td>3.457703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85132</th>\n",
       "      <td>63636_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>3.378214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187305</th>\n",
       "      <td>41640_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>need to write</td>\n",
       "      <td>3.044535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170974</th>\n",
       "      <td>37608_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>time permits</td>\n",
       "      <td>2.987860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326213</th>\n",
       "      <td>7366_6</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>intrigue the reader</td>\n",
       "      <td>2.705677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134501</th>\n",
       "      <td>63712_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>2.589904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269495</th>\n",
       "      <td>28878_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>organise paragraphs</td>\n",
       "      <td>2.516064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134502</th>\n",
       "      <td>63712_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>2.330914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214937</th>\n",
       "      <td>50182_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>job market</td>\n",
       "      <td>2.297615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85133</th>\n",
       "      <td>63636_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>2.252142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134503</th>\n",
       "      <td>63712_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>2.244584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134504</th>\n",
       "      <td>63712_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.899263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238986</th>\n",
       "      <td>28440_5</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.898924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322054</th>\n",
       "      <td>2350_4</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>type of site</td>\n",
       "      <td>1.802767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130788</th>\n",
       "      <td>63007_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.772979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221076</th>\n",
       "      <td>51039_3</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>pretty overwhelming</td>\n",
       "      <td>1.714881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130789</th>\n",
       "      <td>63007_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.684330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130790</th>\n",
       "      <td>63007_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.595681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387870</th>\n",
       "      <td>18554_3</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>done or achieved</td>\n",
       "      <td>1.546073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374202</th>\n",
       "      <td>17077_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.537928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153574</th>\n",
       "      <td>66240_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>used instead</td>\n",
       "      <td>1.525274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389079</th>\n",
       "      <td>16702_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>pretty strict</td>\n",
       "      <td>1.523424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169902</th>\n",
       "      <td>41637_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>therefore talk</td>\n",
       "      <td>1.523424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174564</th>\n",
       "      <td>38308_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>inclined towards</td>\n",
       "      <td>1.509724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84414</th>\n",
       "      <td>66302_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>describe your future</td>\n",
       "      <td>1.499866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130791</th>\n",
       "      <td>63007_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.418383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187952</th>\n",
       "      <td>42022_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>better way</td>\n",
       "      <td>1.299550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292854</th>\n",
       "      <td>32546_7</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>prompt necessary</td>\n",
       "      <td>1.276847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387869</th>\n",
       "      <td>18554_3</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>attraction to chemical</td>\n",
       "      <td>1.230996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174563</th>\n",
       "      <td>38308_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>science and technology</td>\n",
       "      <td>1.219192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239660</th>\n",
       "      <td>28495_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.129617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310928</th>\n",
       "      <td>5264_3</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>environmental problems</td>\n",
       "      <td>1.085013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364675</th>\n",
       "      <td>15145_2</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td>\n",
       "      <td>1.072934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98848</th>\n",
       "      <td>60509_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>one day</td>\n",
       "      <td>1.066182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52667</th>\n",
       "      <td>77173_1</td>\n",
       "      <td>YAKE</td>\n",
       "      <td>mention names</td>\n",
       "      <td>1.049756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374203</th>\n",
       "      <td>17077_1</td>\n",
       "      <td>TopicalPageRank</td>\n",
       "      <td>_ _ _ _ _ _ _ _</td>\n",
       "      <td>1.025286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc_id              kpe  \\\n",
       "134661  63711_1  TopicalPageRank   \n",
       "134662  63711_1  TopicalPageRank   \n",
       "134663  63711_1  TopicalPageRank   \n",
       "52668   77173_1             YAKE   \n",
       "355310  12081_1             YAKE   \n",
       "222474  51326_1             YAKE   \n",
       "306283   7664_2             YAKE   \n",
       "85128   63636_2  TopicalPageRank   \n",
       "86121   54768_1             YAKE   \n",
       "85129   63636_2  TopicalPageRank   \n",
       "85130   63636_2  TopicalPageRank   \n",
       "85131   63636_2  TopicalPageRank   \n",
       "389080  16702_1             YAKE   \n",
       "85132   63636_2  TopicalPageRank   \n",
       "187305  41640_1             YAKE   \n",
       "170974  37608_1             YAKE   \n",
       "326213   7366_6             YAKE   \n",
       "134501  63712_1  TopicalPageRank   \n",
       "269495  28878_1             YAKE   \n",
       "134502  63712_1  TopicalPageRank   \n",
       "214937  50182_1             YAKE   \n",
       "85133   63636_2  TopicalPageRank   \n",
       "134503  63712_1  TopicalPageRank   \n",
       "134504  63712_1  TopicalPageRank   \n",
       "238986  28440_5  TopicalPageRank   \n",
       "322054   2350_4             YAKE   \n",
       "130788  63007_2  TopicalPageRank   \n",
       "221076  51039_3             YAKE   \n",
       "130789  63007_2  TopicalPageRank   \n",
       "130790  63007_2  TopicalPageRank   \n",
       "387870  18554_3             YAKE   \n",
       "374202  17077_1  TopicalPageRank   \n",
       "153574  66240_1             YAKE   \n",
       "389079  16702_1             YAKE   \n",
       "169902  41637_1             YAKE   \n",
       "174564  38308_1             YAKE   \n",
       "84414   66302_1             YAKE   \n",
       "130791  63007_2  TopicalPageRank   \n",
       "187952  42022_1             YAKE   \n",
       "292854  32546_7             YAKE   \n",
       "387869  18554_3             YAKE   \n",
       "174563  38308_1             YAKE   \n",
       "239660  28495_1  TopicalPageRank   \n",
       "310928   5264_3             YAKE   \n",
       "364675  15145_2  TopicalPageRank   \n",
       "98848   60509_1             YAKE   \n",
       "52667   77173_1             YAKE   \n",
       "374203  17077_1  TopicalPageRank   \n",
       "\n",
       "                                                   phrase     score  \n",
       "134661  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  8.358186  \n",
       "134662  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  7.010091  \n",
       "134663  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  6.740472  \n",
       "52668                                       like the name  5.584342  \n",
       "355310                                          good idea  4.963447  \n",
       "222474                            paragraph only contains  4.719739  \n",
       "306283                                   give the reasons  4.389942  \n",
       "85128   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  3.890064  \n",
       "86121                                    search in google  3.884757  \n",
       "85129   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  3.787694  \n",
       "85130   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  3.685324  \n",
       "85131   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  3.582954  \n",
       "389080                                     uni are pretty  3.457703  \n",
       "85132   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  3.378214  \n",
       "187305                                      need to write  3.044535  \n",
       "170974                                       time permits  2.987860  \n",
       "326213                                intrigue the reader  2.705677  \n",
       "134501  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  2.589904  \n",
       "269495                                organise paragraphs  2.516064  \n",
       "134502  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  2.330914  \n",
       "214937                                         job market  2.297615  \n",
       "85133         _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  2.252142  \n",
       "134503  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  2.244584  \n",
       "134504        _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.899263  \n",
       "238986                  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.898924  \n",
       "322054                                       type of site  1.802767  \n",
       "130788            _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.772979  \n",
       "221076                                pretty overwhelming  1.714881  \n",
       "130789              _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.684330  \n",
       "130790                _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.595681  \n",
       "387870                                   done or achieved  1.546073  \n",
       "374202                            _ _ _ _ _ _ _ _ _ _ _ _  1.537928  \n",
       "153574                                       used instead  1.525274  \n",
       "389079                                      pretty strict  1.523424  \n",
       "169902                                     therefore talk  1.523424  \n",
       "174564                                   inclined towards  1.509724  \n",
       "84414                                describe your future  1.499866  \n",
       "130791                    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.418383  \n",
       "187952                                         better way  1.299550  \n",
       "292854                                   prompt necessary  1.276847  \n",
       "387869                             attraction to chemical  1.230996  \n",
       "174563                             science and technology  1.219192  \n",
       "239660                  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.129617  \n",
       "310928                             environmental problems  1.085013  \n",
       "364675            _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  1.072934  \n",
       "98848                                             one day  1.066182  \n",
       "52667                                       mention names  1.049756  \n",
       "374203                                    _ _ _ _ _ _ _ _  1.025286  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exphrase_df[~(exphrase_df['kpe'] == 'FirstPhrases') & (exphrase_df['score'] > 1)].sort_values('score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "exphrase_df = exphrase_df[exphrase_df.score.between(0,1)] # exclude negative and > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rygu\\Anaconda3\\envs\\scrape\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "exphrase_df['phrase'] = exphrase_df['phrase'].str.replace(r\"[^a-z0-9 ']\",'').str.replace(r\" {2,}\",' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phrase\n",
       "do n't think                   167\n",
       "statement of purpose           165\n",
       "will be able                   139\n",
       "do n't have                    108\n",
       "long term goals                104\n",
       "do n't know                     96\n",
       "masters degree course           90\n",
       "do not have                     89\n",
       "would be better                 85\n",
       "year career plan                83\n",
       "current work experience         83\n",
       "masters degree student          78\n",
       "do n't need                     76\n",
       "masters degree studies          74\n",
       "am not sure                     66\n",
       "do not need                     61\n",
       "short term goals                60\n",
       "essay is good                   56\n",
       "be more specific                56\n",
       "future career plans             51\n",
       "'m not sure                     49\n",
       "is very good                    49\n",
       "should be able                  48\n",
       "specific program help           47\n",
       "actual work experience          46\n",
       "college application essay       45\n",
       "would be best                   45\n",
       "do n't want                     44\n",
       "relevant work experience        44\n",
       "extra curricular activities     42\n",
       "is not necessary                40\n",
       "essay is great                  39\n",
       "is very impressive              36\n",
       "admissions advice online        35\n",
       "do n't forget                   34\n",
       "do not think                    34\n",
       "maximum word count              33\n",
       "do n't use                      33\n",
       "long term career goals          33\n",
       "does not have                   31\n",
       "few more sentences              31\n",
       "admissions essay advice         30\n",
       "university can help             30\n",
       "high school student             30\n",
       "do n't worry                    30\n",
       "do n't really need              30\n",
       "would be great                  29\n",
       "write very well                 29\n",
       "do n't like                     29\n",
       "years work experience           28\n",
       "do not use                      28\n",
       "do n't know how                 28\n",
       "there is nothing                28\n",
       "do n't get                      27\n",
       "whole first paragraph           26\n",
       "final year project              25\n",
       "fellow grad students            25\n",
       "long term plans                 25\n",
       "why do n't                      25\n",
       "is too long                     24\n",
       "prompt is asking                24\n",
       "essay is excellent              23\n",
       "have been working               23\n",
       "would be beneficial             23\n",
       "current second paragraph        23\n",
       "is not clear                    23\n",
       "do n't say                      23\n",
       "essay will be                   23\n",
       "next few years                  23\n",
       "would be good                   23\n",
       "long term goal                  22\n",
       "is so impressive                22\n",
       "future career goals             22\n",
       "program will help               22\n",
       "do n't understand               21\n",
       "few minor things                21\n",
       "do not want                     21\n",
       "essay should be                 21\n",
       "do not know                     21\n",
       "post study plan                 20\n",
       "supply chain management         20\n",
       "first few paragraphs            20\n",
       "there should be                 20\n",
       "well written essay              20\n",
       "few grammatical errors          20\n",
       "computer science field          20\n",
       "is not enough                   20\n",
       "post study plans                20\n",
       "program would help              20\n",
       "should not be                   20\n",
       "very good essay                 19\n",
       "does not need                   19\n",
       "personal statement essay        19\n",
       "current work position           19\n",
       "may not be                      19\n",
       "masters degree application      19\n",
       "sentence is too long            18\n",
       "native english speaker          18\n",
       "short term goal                 18\n",
       "will be better                  18\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exphrase_df[exphrase_df.phrase.str.split().str.len() > 2].groupby(['phrase']).score.count().sort_values(ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even abstract of context, we can see a few commonly expressed concepts:\n",
    "* long term goals\n",
    "* year career plan\n",
    "* short term goals\n",
    "* future career plans\n",
    "* long term career goals\n",
    "* relevant work experience\n",
    "* maximum word count\n",
    "* is too long\n",
    "* be more specific\n",
    "\n",
    "and many stating some variant of \"don't need\" / \"not necessary\".\n",
    "\n",
    "From this, we can start putting together the overarching themes. \n",
    "\n",
    "Addressing goals seems to be a frequently discussed topic, particularly for long term. The other main notion is omitting irrelevant content in effort to keep the document short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(text, keep_pos=['ADJ','NOUN','VERB','ADV']):\n",
    "    return \" \".join([t.text for t in filter(lambda t: t.pos_ in keep_pos, nlp(text, disable=['parser','ner']))]) # , disable=['parser','ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(a, vocab, n_top_words=8):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-n_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vctzr_params = dict(stop_words=minimal_stops, strip_accents='ascii', ngram_range=(1,3), max_df=0.8, min_df=5)\n",
    "lda_params = dict(n_components=6, max_iter=50, batch_size=64, evaluate_every=1, \n",
    "                  learning_method='online', n_jobs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vctzr = CountVectorizer(**vctzr_params)\n",
    "lda = LatentDirichletAllocation(**lda_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = count_vctzr.fit_transform(rmcrits)\n",
    "ct_ftnames = [*map(lambda x: x.replace(' ','_'),count_vctzr.get_feature_names())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 50, perplexity: 4908.5938\n",
      "iteration: 2 of max_iter: 50, perplexity: 4363.2268\n",
      "iteration: 3 of max_iter: 50, perplexity: 4252.7059\n",
      "iteration: 4 of max_iter: 50, perplexity: 4206.4561\n",
      "iteration: 5 of max_iter: 50, perplexity: 4181.2662\n",
      "iteration: 6 of max_iter: 50, perplexity: 4165.5252\n",
      "iteration: 7 of max_iter: 50, perplexity: 4154.5851\n",
      "iteration: 8 of max_iter: 50, perplexity: 4146.6657\n",
      "iteration: 9 of max_iter: 50, perplexity: 4140.7530\n",
      "iteration: 10 of max_iter: 50, perplexity: 4136.1232\n",
      "iteration: 11 of max_iter: 50, perplexity: 4132.3841\n",
      "iteration: 12 of max_iter: 50, perplexity: 4129.3784\n",
      "iteration: 13 of max_iter: 50, perplexity: 4126.8868\n",
      "iteration: 14 of max_iter: 50, perplexity: 4124.7465\n",
      "iteration: 15 of max_iter: 50, perplexity: 4122.8814\n",
      "iteration: 16 of max_iter: 50, perplexity: 4121.2369\n",
      "iteration: 17 of max_iter: 50, perplexity: 4119.7834\n",
      "iteration: 18 of max_iter: 50, perplexity: 4118.4761\n",
      "iteration: 19 of max_iter: 50, perplexity: 4117.3116\n",
      "iteration: 20 of max_iter: 50, perplexity: 4116.2962\n",
      "iteration: 21 of max_iter: 50, perplexity: 4115.3744\n",
      "iteration: 22 of max_iter: 50, perplexity: 4114.5320\n",
      "iteration: 23 of max_iter: 50, perplexity: 4113.7410\n",
      "iteration: 24 of max_iter: 50, perplexity: 4113.0448\n",
      "iteration: 25 of max_iter: 50, perplexity: 4112.3976\n",
      "iteration: 26 of max_iter: 50, perplexity: 4111.7692\n",
      "iteration: 27 of max_iter: 50, perplexity: 4111.1828\n",
      "iteration: 28 of max_iter: 50, perplexity: 4110.6379\n",
      "iteration: 29 of max_iter: 50, perplexity: 4110.1401\n",
      "iteration: 30 of max_iter: 50, perplexity: 4109.7182\n",
      "iteration: 31 of max_iter: 50, perplexity: 4109.3118\n",
      "iteration: 32 of max_iter: 50, perplexity: 4108.9232\n",
      "iteration: 33 of max_iter: 50, perplexity: 4108.5350\n",
      "iteration: 34 of max_iter: 50, perplexity: 4108.1726\n",
      "iteration: 35 of max_iter: 50, perplexity: 4107.8214\n",
      "iteration: 36 of max_iter: 50, perplexity: 4107.5014\n",
      "iteration: 37 of max_iter: 50, perplexity: 4107.1900\n",
      "iteration: 38 of max_iter: 50, perplexity: 4106.9178\n",
      "iteration: 39 of max_iter: 50, perplexity: 4106.6726\n",
      "iteration: 40 of max_iter: 50, perplexity: 4106.4446\n",
      "iteration: 41 of max_iter: 50, perplexity: 4106.2225\n",
      "iteration: 42 of max_iter: 50, perplexity: 4106.0025\n",
      "iteration: 43 of max_iter: 50, perplexity: 4105.7709\n",
      "iteration: 44 of max_iter: 50, perplexity: 4105.5186\n",
      "iteration: 45 of max_iter: 50, perplexity: 4105.2730\n",
      "iteration: 46 of max_iter: 50, perplexity: 4105.0624\n",
      "iteration: 47 of max_iter: 50, perplexity: 4104.8732\n",
      "iteration: 48 of max_iter: 50, perplexity: 4104.6948\n",
      "iteration: 49 of max_iter: 50, perplexity: 4104.5102\n",
      "iteration: 50 of max_iter: 50, perplexity: 4104.3482\n"
     ]
    }
   ],
   "source": [
    "Xlda = lda.fit_transform(cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['statement personal not information purpose reviewer personal_statement university prompt statement_purpose letter why application should academic',\n",
       " 'goals want sop why do program you_want school talk them future why_you term plan help',\n",
       " 'work career can research experience masters field university course study studies degree help not professional',\n",
       " 'engineering reader end idea write main very letter thesis design interest topic theme motivation end_first',\n",
       " 'not can if paragraph do if_you more you_can need should just make don because then',\n",
       " 'sentence think good but like not more would me some very can first paragraph also']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(lda.components_, ct_ftnames, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_picklegz(model, count_vectorizer, filepath='saves/models/tmp.pkl.gz'):\n",
    "    \"\"\"Save an LDA model in the format required by pke \"\"\"\n",
    "    ldabunch = (count_vectorizer.get_feature_names(),\n",
    "                model.components_,\n",
    "                model.exp_dirichlet_component_,\n",
    "                model.doc_topic_prior_)\n",
    "    with gzip.open(filepath,'wb') as f:\n",
    "        pickle.dump(ldabunch,f)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saves/models/sklda.pkl.gz'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_picklegz(lda, count_vctzr, 'saves/models/sklda.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = exdoc.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Editor's choice awards:\n",
    "TopicalPageRank: 'avoid flowery statements', 'bland introduction cheesy'\n",
    "PositionRank 'avoid flowery statements', 'tad corny'\n",
    "SingleRank: 'writing could be clearer'\n",
    "TextRank: 'writing could be clearer','eliminate unnecessary wording such'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/sumy/https://pypi.org/project/sumy/\n",
    "def summerize_all(exdoc, nsents=3, lang='english'):\n",
    "    parser = (PlaintextParser.from_file(exdoc.as_posix(), Tokenizer(lang)) \n",
    "              if os.path.isfile(exdoc) \n",
    "              else PlaintextParser.from_string(exdoc, Tokenizer(lang)))\n",
    "    stemmer = Stemmer(lang)\n",
    "    \n",
    "    summ_models = [lsa.LsaSummarizer(stemmer),lex_rank.LexRankSummarizer(stemmer),\n",
    "                   text_rank.TextRankSummarizer(stemmer),kl.KLSummarizer(stemmer),\n",
    "                   luhn.LuhnSummarizer(stemmer),reduction.ReductionSummarizer(stemmer)]\n",
    "    \n",
    "    for summzr in summ_models:\n",
    "        summzr.stop_words = minimal_stops\n",
    "        name = str(summzr.__class__).split('.')[-1][:-2]\n",
    "        print(f\"===== {name} =====\")\n",
    "        for sentence in summzr(parser.document, nsents):\n",
    "            print('>', sentence, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== LsaSummarizer =====\n",
      "> The whole essay should be powerful, as if it is a beautiful explanation of a single profound thought.\n",
      "\n",
      "> Can you inspire the reader with an insight into film that makes it, perhaps, the most meaningful pursuit possible!?\n",
      "\n",
      "> After all, it is the most technologically sophisticated form of modern art -- and it really does include all the arts, from writing to music to storytelling and more.\n",
      "\n",
      "> Can you take this as an opportunity to express an important idea of yours, without interruption?\n",
      "\n",
      "> The body of the essay, with your info and accomplishments, must be presented in a way that supports your main idea -- which may be a philosophical point about film.\n",
      "\n",
      "===== LexRankSummarizer =====\n",
      "> The rule is like this: Say it, explain it, and then say it again.\n",
      "\n",
      "> The whole essay should be powerful, as if it is a beautiful explanation of a single profound thought.\n",
      "\n",
      "> Can you inspire the reader with an insight into film that makes it, perhaps, the most meaningful pursuit possible!?\n",
      "\n",
      "> After all, it is the most technologically sophisticated form of modern art -- and it really does include all the arts, from writing to music to storytelling and more.\n",
      "\n",
      "> Can you take this as an opportunity to express an important idea of yours, without interruption?\n",
      "\n",
      "===== TextRankSummarizer =====\n",
      "> The rule is like this: Say it, explain it, and then say it again.\n",
      "\n",
      "> The whole essay should be powerful, as if it is a beautiful explanation of a single profound thought.\n",
      "\n",
      "> Can you inspire the reader with an insight into film that makes it, perhaps, the most meaningful pursuit possible!?\n",
      "\n",
      "> After all, it is the most technologically sophisticated form of modern art -- and it really does include all the arts, from writing to music to storytelling and more.\n",
      "\n",
      "> Tell the main idea in the first paragraph, lead the reader through a summary of your experience in a few paragraphs, and restate the main idea in the closing paragraph.\n",
      "\n",
      "===== KLSummarizer =====\n",
      "> The rule is like this: Say it, explain it, and then say it again.\n",
      "\n",
      "> The whole essay should be powerful, as if it is a beautiful explanation of a single profound thought.\n",
      "\n",
      "> Tell the main idea in the first paragraph, lead the reader through a summary of your experience in a few paragraphs, and restate the main idea in the closing paragraph.\n",
      "\n",
      "> The body of the essay, with your info and accomplishments, must be presented in a way that supports your main idea -- which may be a philosophical point about film.\n",
      "\n",
      "> !\n",
      "\n",
      "===== LuhnSummarizer =====\n",
      "> Can you inspire the reader with an insight into film that makes it, perhaps, the most meaningful pursuit possible!?\n",
      "\n",
      "> After all, it is the most technologically sophisticated form of modern art -- and it really does include all the arts, from writing to music to storytelling and more.\n",
      "\n",
      "> Can you take this as an opportunity to express an important idea of yours, without interruption?\n",
      "\n",
      "> Tell the main idea in the first paragraph, lead the reader through a summary of your experience in a few paragraphs, and restate the main idea in the closing paragraph.\n",
      "\n",
      "> The body of the essay, with your info and accomplishments, must be presented in a way that supports your main idea -- which may be a philosophical point about film.\n",
      "\n",
      "===== ReductionSummarizer =====\n",
      "> Can you inspire the reader with an insight into film that makes it, perhaps, the most meaningful pursuit possible!?\n",
      "\n",
      "> Can you take this as an opportunity to express an important idea of yours, without interruption?\n",
      "\n",
      "> Tell the main idea in the first paragraph, lead the reader through a summary of your experience in a few paragraphs, and restate the main idea in the closing paragraph.\n",
      "\n",
      "> The body of the essay, with your info and accomplishments, must be presented in a way that supports your main idea -- which may be a philosophical point about film.\n",
      "\n",
      "> I hope that helps you get started!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summerize_all(samp.iloc[0], nsents=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
