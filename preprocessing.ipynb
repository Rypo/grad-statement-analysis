{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has undergone a multitude of different preprocessing steps. Rather than scattering them across all the attempts, they will all be mentioned here.\n",
    "\n",
    "The main section headings will denote different preprocessing attempts. This notebook is not cohesive across sections, it only serves to document various attempts throughout the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V0 - Initial Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows the first methods used for preprocessing. It uses a subsection of the dataset that only includes the word \"data\" in the `post_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import string\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "# Ignore smart open warning until update pushed to conda: https://github.com/RaRe-Technologies/gensim/pull/2530\n",
    "warnings.filterwarnings('ignore',message='.+smart_open.+')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "from FaiText import Tokenizer, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "#nlp.disable_pipes('parser','ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>post_content</th>\n",
       "      <th>post_date</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>slug</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_likes</th>\n",
       "      <th>user_posts</th>\n",
       "      <th>user_threads</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guiaria</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;&lt;h2&gt;data field - the area tha...</td>\n",
       "      <td>May 15, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>statement-applying-german-university-master-83160</td>\n",
       "      <td>Statement of Purpose for applying to German Un...</td>\n",
       "      <td>83160</td>\n",
       "      <td>113287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>guiaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria, EF Contrib</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;@guiaria&lt;br/&gt;Hi there! Let's ...</td>\n",
       "      <td>May 15, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>statement-applying-german-university-master-83160</td>\n",
       "      <td>Statement of Purpose for applying to German Un...</td>\n",
       "      <td>83160</td>\n",
       "      <td>112562.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>630</td>\n",
       "      <td>-</td>\n",
       "      <td>Maria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>İlkay Albayrak</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;Hello everyone, I am currentl...</td>\n",
       "      <td>Apr 30, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>great-investment-future-motivation-msc-data-83059</td>\n",
       "      <td>'great investment for my future' - Motivation ...</td>\n",
       "      <td>83059</td>\n",
       "      <td>113040.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dreiframe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                       post_content  \\\n",
       "0            guiaria  <div class=\"pTx\"><h2>data field - the area tha...   \n",
       "1  Maria, EF Contrib  <div class=\"pTx\">@guiaria<br/>Hi there! Let's ...   \n",
       "2     İlkay Albayrak  <div class=\"pTx\">Hello everyone, I am currentl...   \n",
       "\n",
       "      post_date  seqnum                                               slug  \\\n",
       "0  May 15, 2019       0  statement-applying-german-university-master-83160   \n",
       "1  May 15, 2019       1  statement-applying-german-university-master-83160   \n",
       "2  Apr 30, 2019       0  great-investment-future-motivation-msc-data-83059   \n",
       "\n",
       "                                        thread_title  url_id   user_id  \\\n",
       "0  Statement of Purpose for applying to German Un...   83160  113287.0   \n",
       "1  Statement of Purpose for applying to German Un...   83160  112562.0   \n",
       "2  'great investment for my future' - Motivation ...   83059  113040.0   \n",
       "\n",
       "   user_likes user_posts user_threads   username  \n",
       "0         NaN          -            1    guiaria  \n",
       "1       248.0        630            -      Maria  \n",
       "2         NaN          1            1  dreiframe  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpath = Path('grad_scrape/gradsop/data')\n",
    "sop_data = pd.read_csv(dpath/'data_posts_all.csv')\n",
    "sop_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite personally creating this dataset, it still requires some additional processing before use. I opted to not strip out html from the text to avoid additional processing time while scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "sop_data.loc[:, ['user_posts','user_threads']] = sop_data.loc[:, ['user_posts','user_threads']].apply(lambda x: x.str.replace(',','').replace('-','0')).fillna(0).astype(np.int)\n",
    "sop_data['user_likes'] = sop_data.user_likes.fillna(0)\n",
    "sop_data['user_id'] = sop_data.user_id.fillna(0).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>post_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83160_0</td>\n",
       "      <td>83160</td>\n",
       "      <td>113287</td>\n",
       "      <td>0</td>\n",
       "      <td>data field - the area that attracts me the mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83160_1</td>\n",
       "      <td>83160</td>\n",
       "      <td>112562</td>\n",
       "      <td>1</td>\n",
       "      <td>@guiaria\\nHi there! Let's work through your es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83059_0</td>\n",
       "      <td>83059</td>\n",
       "      <td>113040</td>\n",
       "      <td>0</td>\n",
       "      <td>Hello everyone, I am currently going through a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83059_1</td>\n",
       "      <td>83059</td>\n",
       "      <td>112562</td>\n",
       "      <td>1</td>\n",
       "      <td>While the formality of your introductory parag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83059_2</td>\n",
       "      <td>83059</td>\n",
       "      <td>113040</td>\n",
       "      <td>2</td>\n",
       "      <td>Thank you so much Maria!\\nThis is really helpf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id  url_id  user_id  seqnum  \\\n",
       "0  83160_0   83160   113287       0   \n",
       "1  83160_1   83160   112562       1   \n",
       "2  83059_0   83059   113040       0   \n",
       "3  83059_1   83059   112562       1   \n",
       "4  83059_2   83059   113040       2   \n",
       "\n",
       "                                        post_content  \n",
       "0  data field - the area that attracts me the mos...  \n",
       "1  @guiaria\\nHi there! Let's work through your es...  \n",
       "2  Hello everyone, I am currently going through a...  \n",
       "3  While the formality of your introductory parag...  \n",
       "4  Thank you so much Maria!\\nThis is really helpf...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = sop_data.copy()\n",
    "# Clean html\n",
    "df_text['post_content'] = df_text['post_content'].str.replace('<br/>','\\n').apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "# assign unique id to each document\n",
    "df_text['doc_id'] = df_text['url_id'].astype(str) + '_' + df_text['seqnum'].astype(str)\n",
    "df_text = df_text[['doc_id','url_id','user_id','seqnum','post_content']]\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>post_content</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url_id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">67923</th>\n",
       "      <th>0</th>\n",
       "      <td>Could any one help me with the structure?\\n\\nA...</td>\n",
       "      <td>96760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi Serena, as I read and understand your essay...</td>\n",
       "      <td>91533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi, thank you so much for your advice, you kno...</td>\n",
       "      <td>96760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi Serena, indeed, it is hard to write let alo...</td>\n",
       "      <td>91533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">67973</th>\n",
       "      <th>0</th>\n",
       "      <td>Hi, everyone.\\n\\nI am currently applying to Ba...</td>\n",
       "      <td>96803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi Akmal, as I read and reviewed your essay, I...</td>\n",
       "      <td>91533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi, @justivy03! Thank you for your thoughtful ...</td>\n",
       "      <td>96803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">68018</th>\n",
       "      <th>0</th>\n",
       "      <td>Hi everyone, I have written a statement of pur...</td>\n",
       "      <td>96869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi Tommaso, I have read a few paragraphs of yo...</td>\n",
       "      <td>91533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Justivy03, thank you for your feedback! How...</td>\n",
       "      <td>96869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    post_content  user_id\n",
       "url_id seqnum                                                            \n",
       "67923  0       Could any one help me with the structure?\\n\\nA...    96760\n",
       "       1       Hi Serena, as I read and understand your essay...    91533\n",
       "       2       Hi, thank you so much for your advice, you kno...    96760\n",
       "       3       Hi Serena, indeed, it is hard to write let alo...    91533\n",
       "67973  0       Hi, everyone.\\n\\nI am currently applying to Ba...    96803\n",
       "       1       Hi Akmal, as I read and reviewed your essay, I...    91533\n",
       "       2       Hi, @justivy03! Thank you for your thoughtful ...    96803\n",
       "68018  0       Hi everyone, I have written a statement of pur...    96869\n",
       "       1       Hi Tommaso, I have read a few paragraphs of yo...    91533\n",
       "       2       Hi Justivy03, thank you for your feedback! How...    96869"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seqnum = 0 is the original post, subsequent numbers are the responses\n",
    "df_text.pivot_table(['post_content','user_id'],['url_id','seqnum'],aggfunc=lambda x: x).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all instances where OP added a reply to their post. \n",
    "\n",
    "```\n",
    "OP is when seqnum == 0;\n",
    "get user_id for OP;\n",
    "find where user_id = OP.user_id and seqnum != 0;\n",
    "if any, check if url_id is same;\n",
    "when true, OP has replied to own post\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>seqnum</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67923</th>\n",
       "      <td>96760.0</td>\n",
       "      <td>91533.0</td>\n",
       "      <td>96760.0</td>\n",
       "      <td>91533.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67973</th>\n",
       "      <td>96803.0</td>\n",
       "      <td>91533.0</td>\n",
       "      <td>96803.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68018</th>\n",
       "      <td>96869.0</td>\n",
       "      <td>91533.0</td>\n",
       "      <td>96869.0</td>\n",
       "      <td>91533.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70237</th>\n",
       "      <td>97454.0</td>\n",
       "      <td>98377.0</td>\n",
       "      <td>97454.0</td>\n",
       "      <td>91533.0</td>\n",
       "      <td>97454.0</td>\n",
       "      <td>91533.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70641</th>\n",
       "      <td>98742.0</td>\n",
       "      <td>98707.0</td>\n",
       "      <td>98742.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "seqnum       0        1        2        3        4        5   6   7   8   9   \\\n",
       "url_id                                                                         \n",
       "67923   96760.0  91533.0  96760.0  91533.0      NaN      NaN NaN NaN NaN NaN   \n",
       "67973   96803.0  91533.0  96803.0      NaN      NaN      NaN NaN NaN NaN NaN   \n",
       "68018   96869.0  91533.0  96869.0  91533.0      NaN      NaN NaN NaN NaN NaN   \n",
       "70237   97454.0  98377.0  97454.0  91533.0  97454.0  91533.0 NaN NaN NaN NaN   \n",
       "70641   98742.0  98707.0  98742.0      NaN      NaN      NaN NaN NaN NaN NaN   \n",
       "\n",
       "seqnum  ...  19  20  21  22  23  24  25  26  27  28  \n",
       "url_id  ...                                          \n",
       "67923   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "67973   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "68018   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "70237   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "70641   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_piv = df_text.pivot('url_id','seqnum','user_id')\n",
    "op_multi = user_piv[user_piv.apply(lambda x: x[0] in x[1:].values,axis=1)]\n",
    "op_ids = op_multi.loc[:,0]\n",
    "op_multi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Approx. 1.5min runtime\n",
    "docs = df_text.post_content.apply(nlp)\n",
    "df_text['tokens'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df_text.to_pickle('proc_data/data_all_df_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 primary dataframes we will be using throughout the analysis. Representing\n",
    "- all_text: Full texts (original posts + all responses to the posts)\n",
    "- op_post: The initial thread post\n",
    "- op_resp: Instances where the creator of a thread responds to their own thread\n",
    "- critiques: All non-creator responses to a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_pickle('proc_data/data_all_df_text.pkl')\n",
    "#df_text = pickle.load(open('saves/data_all_df_text.pkl','rb'))\n",
    "df_origpost = df_text.query('seqnum == 0')\n",
    "df_op_reply = df_text.query('url_id.isin(@op_multi.index) & user_id.isin(@op_ids) & seqnum != 0')\n",
    "df_critiques = df_text.drop(df_op_reply.index).query('seqnum > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>post_content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>73925_3</td>\n",
       "      <td>73925</td>\n",
       "      <td>100360</td>\n",
       "      <td>3</td>\n",
       "      <td>Hi please evaluate these two sections:\\nIntrod...</td>\n",
       "      <td>(Hi, please, evaluate, these, two, sections, :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>75109_2</td>\n",
       "      <td>75109</td>\n",
       "      <td>101299</td>\n",
       "      <td>2</td>\n",
       "      <td>I have chosen to apply to the University of Am...</td>\n",
       "      <td>(I, have, chosen, to, apply, to, the, Universi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>75090_21</td>\n",
       "      <td>75090</td>\n",
       "      <td>101268</td>\n",
       "      <td>21</td>\n",
       "      <td>@Holt for thesis course plzz review this\\nDuri...</td>\n",
       "      <td>(@Holt, for, thesis, course, plzz, review, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>72896_2</td>\n",
       "      <td>72896</td>\n",
       "      <td>99871</td>\n",
       "      <td>2</td>\n",
       "      <td>@Holt\\n\\nHi Holt, I really appreciate your fee...</td>\n",
       "      <td>(@Holt, \\n\\n, Hi, Holt, ,, I, really, apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>72896_4</td>\n",
       "      <td>72896</td>\n",
       "      <td>99871</td>\n",
       "      <td>4</td>\n",
       "      <td>@Holt\\n\\nHi Holt, I appreciate your suggestion...</td>\n",
       "      <td>(@Holt, \\n\\n, Hi, Holt, ,, I, appreciate, your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>79397_2</td>\n",
       "      <td>79397</td>\n",
       "      <td>106917</td>\n",
       "      <td>2</td>\n",
       "      <td>STATEMENT OF PURPOSE\\n\\nI have been working in...</td>\n",
       "      <td>(STATEMENT, OF, PURPOSE, \\n\\n, I, have, been, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>81045_3</td>\n",
       "      <td>81045</td>\n",
       "      <td>109897</td>\n",
       "      <td>3</td>\n",
       "      <td>Thanks for the inpust komziee and Holt\\n\\n@Hol...</td>\n",
       "      <td>(Thanks, for, the, inpust, komziee, and, Holt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>81613_2</td>\n",
       "      <td>81613</td>\n",
       "      <td>110661</td>\n",
       "      <td>2</td>\n",
       "      <td>Dear @Holt , thank you so much for your feedba...</td>\n",
       "      <td>(Dear, @Holt, ,, thank, you, so, much, for, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>81613_5</td>\n",
       "      <td>81613</td>\n",
       "      <td>110661</td>\n",
       "      <td>5</td>\n",
       "      <td>Dear @Holt this is my revision. As much as I c...</td>\n",
       "      <td>(Dear, @Holt, this, is, my, revision, ., As, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>82046_2</td>\n",
       "      <td>82046</td>\n",
       "      <td>111372</td>\n",
       "      <td>2</td>\n",
       "      <td>Great thanks for your advice, Mary! I also wor...</td>\n",
       "      <td>(Great, thanks, for, your, advice, ,, Mary, !,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id  url_id  user_id  seqnum  \\\n",
       "50    73925_3   73925   100360       3   \n",
       "55    75109_2   75109   101299       2   \n",
       "97   75090_21   75090   101268      21   \n",
       "154   72896_2   72896    99871       2   \n",
       "158   72896_4   72896    99871       4   \n",
       "218   79397_2   79397   106917       2   \n",
       "247   81045_3   81045   109897       3   \n",
       "278   81613_2   81613   110661       2   \n",
       "282   81613_5   81613   110661       5   \n",
       "352   82046_2   82046   111372       2   \n",
       "\n",
       "                                          post_content  \\\n",
       "50   Hi please evaluate these two sections:\\nIntrod...   \n",
       "55   I have chosen to apply to the University of Am...   \n",
       "97   @Holt for thesis course plzz review this\\nDuri...   \n",
       "154  @Holt\\n\\nHi Holt, I really appreciate your fee...   \n",
       "158  @Holt\\n\\nHi Holt, I appreciate your suggestion...   \n",
       "218  STATEMENT OF PURPOSE\\n\\nI have been working in...   \n",
       "247  Thanks for the inpust komziee and Holt\\n\\n@Hol...   \n",
       "278  Dear @Holt , thank you so much for your feedba...   \n",
       "282  Dear @Holt this is my revision. As much as I c...   \n",
       "352  Great thanks for your advice, Mary! I also wor...   \n",
       "\n",
       "                                                tokens  \n",
       "50   (Hi, please, evaluate, these, two, sections, :...  \n",
       "55   (I, have, chosen, to, apply, to, the, Universi...  \n",
       "97   (@Holt, for, thesis, course, plzz, review, thi...  \n",
       "154  (@Holt, \\n\\n, Hi, Holt, ,, I, really, apprecia...  \n",
       "158  (@Holt, \\n\\n, Hi, Holt, ,, I, appreciate, your...  \n",
       "218  (STATEMENT, OF, PURPOSE, \\n\\n, I, have, been, ...  \n",
       "247  (Thanks, for, the, inpust, komziee, and, Holt,...  \n",
       "278  (Dear, @Holt, ,, thank, you, so, much, for, yo...  \n",
       "282  (Dear, @Holt, this, is, my, revision, ., As, m...  \n",
       "352  (Great, thanks, for, your, advice, ,, Mary, !,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_op_reply[df_op_reply['post_content'].apply(len) > 1000].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few instances where the original poster revised something and asked again for advice. To simplify things, we will disregard the information for the time being and limit replies purely to those by other individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 - Custom HTML tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, I attempted to create a custom tokenizer that added in new token values based on html markup. The premise was similar to the approach that Fast.ai employees, so it seemed like a reasonable class to extend.\n",
    "\n",
    "This version of the corpus was used for building a language model.\n",
    "\n",
    "Much of the interesting code for this section is in separate files\n",
    "* `FaiText.py` - A standalone version of Fast.ai's text module.\n",
    "* `HTMLutils.py` - The custom html tokenizing functions, stylized in a similar fashion to fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import string\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode\n",
    "# Ignore smart open warning until update pushed to conda: https://github.com/RaRe-Technologies/gensim/pull/2530\n",
    "warnings.filterwarnings('ignore',message='.+smart_open.+')\n",
    "import spacy\n",
    "\n",
    "from FaiText import Tokenizer, defaults\n",
    "import HTMLutils as HU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>post_content</th>\n",
       "      <th>post_date</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>slug</th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_likes</th>\n",
       "      <th>user_posts</th>\n",
       "      <th>user_threads</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeu Deck Ngui</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;Hi guys, I would need help re...</td>\n",
       "      <td>Mar 16, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>study-plan-canada-permit-meng-civil-env-82750</td>\n",
       "      <td>82750</td>\n",
       "      <td>112438.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>decimo1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary Rose</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;Yeu, I am afraid that this is...</td>\n",
       "      <td>Mar 17, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>study-plan-canada-permit-meng-civil-env-82750</td>\n",
       "      <td>82750</td>\n",
       "      <td>99412.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>7,529</td>\n",
       "      <td>-</td>\n",
       "      <td>Holt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snow</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;&lt;span class=\"b\"&gt;Please confin...</td>\n",
       "      <td>Mar 17, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>application-statement-msc-electronic-82755</td>\n",
       "      <td>82755</td>\n",
       "      <td>112507.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>Airydisc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nkitha</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;I really need help in this pa...</td>\n",
       "      <td>Mar 21, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>goal-study-plan-korean-university-82784</td>\n",
       "      <td>82784</td>\n",
       "      <td>112553.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>itha20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Constance, EF Contrib</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;Since this is an academic pap...</td>\n",
       "      <td>Mar 22, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>application-statement-msc-electronic-82755</td>\n",
       "      <td>82755</td>\n",
       "      <td>112560.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19</td>\n",
       "      <td>-</td>\n",
       "      <td>Constance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                       post_content  \\\n",
       "0          Yeu Deck Ngui  <div class=\"pTx\">Hi guys, I would need help re...   \n",
       "1              Mary Rose  <div class=\"pTx\">Yeu, I am afraid that this is...   \n",
       "2                   snow  <div class=\"pTx\"><span class=\"b\">Please confin...   \n",
       "3                 nkitha  <div class=\"pTx\">I really need help in this pa...   \n",
       "4  Constance, EF Contrib  <div class=\"pTx\">Since this is an academic pap...   \n",
       "\n",
       "      post_date  seqnum                                           slug  \\\n",
       "0  Mar 16, 2019       0  study-plan-canada-permit-meng-civil-env-82750   \n",
       "1  Mar 17, 2019       1  study-plan-canada-permit-meng-civil-env-82750   \n",
       "2  Mar 17, 2019       0     application-statement-msc-electronic-82755   \n",
       "3  Mar 21, 2019       0        goal-study-plan-korean-university-82784   \n",
       "4  Mar 22, 2019       1     application-statement-msc-electronic-82755   \n",
       "\n",
       "   url_id   user_id  user_likes user_posts user_threads    username  \n",
       "0   82750  112438.0         NaN          -            1  decimo1993  \n",
       "1   82750   99412.0      2001.0      7,529            -        Holt  \n",
       "2   82755  112507.0         NaN          -            1    Airydisc  \n",
       "3   82784  112553.0         NaN          -            1      itha20  \n",
       "4   82755  112560.0         9.0         19            -   Constance  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpath = Path('grad_scrape/gradsop/data')\n",
    "\n",
    "sop_data = pd.read_csv(dpath/'posts_all.csv')\n",
    "\n",
    "sop_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "sop_data.loc[:, ['user_posts','user_threads']] = sop_data.loc[:, ['user_posts','user_threads']].apply(lambda x: x.str.replace(',','').replace('-','0')).fillna(0).astype(np.int)\n",
    "sop_data['user_likes'] = sop_data.user_likes.fillna(0)\n",
    "sop_data['user_id'] = sop_data.user_id.fillna(0).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<span class=\"q\"> -> xxquo\n",
    "<del> -> xxdel\n",
    "<span class=\"r\">,<span class=\"b\">,<span class=\"g\"> -> xxhl (or xxred, xxblu,xxgrn)\n",
    "<b>,<strong> -> xxbld\n",
    "<em>,<i> -> xxitl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function FaiText.fix_html(x: str) -> str>,\n",
       " <function FaiText.replace_rep(t: str) -> str>,\n",
       " <function FaiText.replace_wrep(t: str) -> str>,\n",
       " <function FaiText.spec_add_spaces(t: str) -> str>,\n",
       " <function FaiText.rm_useless_spaces(t: str) -> str>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_pre_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = Tokenizer(\n",
    "    pre_rules=[HU.HTMLTokenizer.tokenize, fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces],\n",
    "    post_rules = [replace_all_caps, deal_caps],\n",
    "    special_cases=defaults.text_spec_tok+HU.CSPEC_CASE,\n",
    "    n_cpus=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"pTx\"><span class=\"b\">Please confine your statement to no more than 300 words. The online form can accept English characters only.<br/>The content of your statement should explain why you wish to study the programme and how the qualification is relevant to your career aspirations, as well as your expectation of the programme. If applicable, provide other information (e.g. work experience, non-academic achievements, community services) that you think is relevant to the assessment of your application.</span><br/><br/>Application Statement<br/><br/>Much of my motivation for pursuing MSc Electronic Information Engineering provided by CityU originates from an intense desire to take my academic attainment to the next level and to prepare me for a successful career. As a matter of fact, I have been keen to study signal processing and wireless communication since childhood. My proudest accomplishment was to assemble a radio receiver with components that I bought online. As my passion grew, I was enrolled in the undergraduate program of Applied Physics at Hefei University of Technology, included in National Project 211 Universities. The past four years witnessed my relentless and effective endeavors to build on a solid foundation in electronic circuit and signaling system both theoretically and practically. As an ambitious student, I am never satisfied with where I am. It is my dream to push the boundaries of electronic information engineering through further studies.<br/><br/>Surely, undertaking the postgraduate programme is to fulfill career aspirations. I would like to divide them into two phases. In the short run, I will continue my research as a PhD candidate to delve into a specific area that accommodates my interest. When it comes to my long-range objective, I will be an electronics engineer at a renowned research institute to get the most out of my knowledge and experience. MSc Electronic Information Engineering will help realize my career goals by offering a wide spectrum of courses designed to deepen my understanding of applied electromagnetism, wireless communications and networking. Besides, after completing the research paper that wants 9 credits, I will be more likely to be accepted into the doctoral programme. Hence, my expectation from the studies in my chosen programme is to get advanced knowledge and gain real-world experience relevant to the current and anticipated future needs from industry.</div>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sop_data.post_content.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxhlb Please confine your statement to no more than 300 words. The online form can accept English characters only.\\nThe content of your statement should explain why you wish to study the programme and how the qualification is relevant to your career aspirations, as well as your expectation of the programme. If applicable, provide other information (e.g. work experience, non-academic achievements, community services) that you think is relevant to the assessment of your application. xxhle \\n\\nApplication Statement\\n\\nMuch of my motivation for pursuing MSc Electronic Information Engineering provided by CityU originates from an intense desire to take my academic attainment to the next level and to prepare me for a successful career. As a matter of fact, I have been keen to study signal processing and wireless communication since childhood. My proudest accomplishment was to assemble a radio receiver with components that I bought online. As my passion grew, I was enrolled in the undergraduate program of Applied Physics at Hefei University of Technology, included in National Project 211 Universities. The past four years witnessed my relentless and effective endeavors to build on a solid foundation in electronic circuit and signaling system both theoretically and practically. As an ambitious student, I am never satisfied with where I am. It is my dream to push the boundaries of electronic information engineering through further studies.\\n\\nSurely, undertaking the postgraduate programme is to fulfill career aspirations. I would like to divide them into two phases. In the short run, I will continue my research as a PhD candidate to delve into a specific area that accommodates my interest. When it comes to my long-range objective, I will be an electronics engineer at a renowned research institute to get the most out of my knowledge and experience. MSc Electronic Information Engineering will help realize my career goals by offering a wide spectrum of courses designed to deepen my understanding of applied electromagnetism, wireless communications and networking. Besides, after completing the research paper that wants 9 credits, I will be more likely to be accepted into the doctoral programme. Hence, my expectation from the studies in my chosen programme is to get advanced knowledge and gain real-world experience relevant to the current and anticipated future needs from industry.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HU.HTMLTokenizer.tokenize(sop_data.post_content.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  xxhlb xxmaj please confine your statement to no more than 300 words . xxmaj the online form can accept xxmaj english characters only . \\n  xxmaj the content of your statement should explain why you wish to study the programme and how the qualification is relevant to your career aspirations , as well as your expectation of the programme . xxmaj if applicable , provide other information ( e.g. work experience , non - academic achievements , community services ) that you think is relevant to the assessment of your application . xxhle \\n \\n  xxmaj application xxmaj statement \\n \\n  xxmaj much of my motivation for pursuing msc xxmaj electronic xxmaj information xxmaj engineering provided by cityu originates from an intense desire to take my academic attainment to the next level and to prepare me for a successful career . xxmaj as a matter of fact , i have been keen to study signal processing and wireless communication since childhood . xxmaj my proudest accomplishment was to assemble a radio receiver with components that i bought online . xxmaj as my passion grew , i was enrolled in the undergraduate program of xxmaj applied xxmaj physics at xxmaj hefei xxmaj university of xxmaj technology , included in xxmaj national xxmaj project 211 xxmaj universities . xxmaj the past four years witnessed my relentless and effective endeavors to build on a solid foundation in electronic circuit and signaling system both theoretically and practically . xxmaj as an ambitious student , i am never satisfied with where i am . xxmaj it is my dream to push the boundaries of electronic information engineering through further studies . \\n \\n  xxmaj surely , undertaking the postgraduate programme is to fulfill career aspirations . i would like to divide them into two phases . xxmaj in the short run , i will continue my research as a phd candidate to delve into a specific area that accommodates my interest . xxmaj when it comes to my long - range objective , i will be an electronics engineer at a renowned research institute to get the most out of my knowledge and experience . msc xxmaj electronic xxmaj information xxmaj engineering will help realize my career goals by offering a wide spectrum of courses designed to deepen my understanding of applied electromagnetism , wireless communications and networking . xxmaj besides , after completing the research paper that wants 9 credits , i will be more likely to be accepted into the doctoral programme . xxmaj hence , my expectation from the studies in my chosen programme is to get advanced knowledge and gain real - world experience relevant to the current and anticipated future needs from industry .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \". join(tknzr.process_all(sop_data.post_content.iloc[:3])[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 - Documents on Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of this section was to put the documents in a format that could easily be transfered to Colab. From there, they were used to train a several language models with the intention of creating a sentiment classifier. The entries ended up being prohibitively long for transformer models but did train with ULMfit.\n",
    "\n",
    "The documents were also used for summarizing and key-phrase extraction models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>post_content</th>\n",
       "      <th>post_date</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>slug</th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_likes</th>\n",
       "      <th>user_posts</th>\n",
       "      <th>user_threads</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeu Deck Ngui</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;Hi guys, I would need help re...</td>\n",
       "      <td>Mar 16, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>study-plan-canada-permit-meng-civil-env-82750</td>\n",
       "      <td>82750</td>\n",
       "      <td>112438.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>decimo1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary Rose</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;Yeu, I am afraid that this is...</td>\n",
       "      <td>Mar 17, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>study-plan-canada-permit-meng-civil-env-82750</td>\n",
       "      <td>82750</td>\n",
       "      <td>99412.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>7,529</td>\n",
       "      <td>-</td>\n",
       "      <td>Holt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snow</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;&lt;span class=\"b\"&gt;Please confin...</td>\n",
       "      <td>Mar 17, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>application-statement-msc-electronic-82755</td>\n",
       "      <td>82755</td>\n",
       "      <td>112507.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>Airydisc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nkitha</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;I really need help in this pa...</td>\n",
       "      <td>Mar 21, 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>goal-study-plan-korean-university-82784</td>\n",
       "      <td>82784</td>\n",
       "      <td>112553.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>itha20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Constance, EF Contrib</td>\n",
       "      <td>&lt;div class=\"pTx\"&gt;Since this is an academic pap...</td>\n",
       "      <td>Mar 22, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>application-statement-msc-electronic-82755</td>\n",
       "      <td>82755</td>\n",
       "      <td>112560.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19</td>\n",
       "      <td>-</td>\n",
       "      <td>Constance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                       post_content  \\\n",
       "0          Yeu Deck Ngui  <div class=\"pTx\">Hi guys, I would need help re...   \n",
       "1              Mary Rose  <div class=\"pTx\">Yeu, I am afraid that this is...   \n",
       "2                   snow  <div class=\"pTx\"><span class=\"b\">Please confin...   \n",
       "3                 nkitha  <div class=\"pTx\">I really need help in this pa...   \n",
       "4  Constance, EF Contrib  <div class=\"pTx\">Since this is an academic pap...   \n",
       "\n",
       "      post_date  seqnum                                           slug  \\\n",
       "0  Mar 16, 2019       0  study-plan-canada-permit-meng-civil-env-82750   \n",
       "1  Mar 17, 2019       1  study-plan-canada-permit-meng-civil-env-82750   \n",
       "2  Mar 17, 2019       0     application-statement-msc-electronic-82755   \n",
       "3  Mar 21, 2019       0        goal-study-plan-korean-university-82784   \n",
       "4  Mar 22, 2019       1     application-statement-msc-electronic-82755   \n",
       "\n",
       "   url_id   user_id  user_likes user_posts user_threads    username  \n",
       "0   82750  112438.0         NaN          -            1  decimo1993  \n",
       "1   82750   99412.0      2001.0      7,529            -        Holt  \n",
       "2   82755  112507.0         NaN          -            1    Airydisc  \n",
       "3   82784  112553.0         NaN          -            1      itha20  \n",
       "4   82755  112560.0         9.0         19            -   Constance  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpath = Path('grad_scrape/gradsop/data')\n",
    "\n",
    "sop_data = pd.read_csv(dpath/'posts_all.csv')\n",
    "\n",
    "sop_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name             True\n",
       "post_content    False\n",
       "post_date       False\n",
       "seqnum          False\n",
       "slug            False\n",
       "url_id          False\n",
       "user_id          True\n",
       "user_likes       True\n",
       "user_posts       True\n",
       "user_threads     True\n",
       "username         True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sop_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "sop_data.loc[:, ['user_posts','user_threads']] = sop_data.loc[:, ['user_posts','user_threads']].apply(lambda x: x.str.replace(',','').replace('-','0')).fillna(0).astype(np.int)\n",
    "sop_data['user_likes'] = sop_data.user_likes.fillna(0)\n",
    "sop_data['user_id'] = sop_data.user_id.fillna(0).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = sop_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>post_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82750_0</td>\n",
       "      <td>82750</td>\n",
       "      <td>112438</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi guys, I would need help reviewing this stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82750_1</td>\n",
       "      <td>82750</td>\n",
       "      <td>99412</td>\n",
       "      <td>1</td>\n",
       "      <td>Yeu, I am afraid that this is not a valid stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82755_0</td>\n",
       "      <td>82755</td>\n",
       "      <td>112507</td>\n",
       "      <td>0</td>\n",
       "      <td>Please confine your statement to no more than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82784_0</td>\n",
       "      <td>82784</td>\n",
       "      <td>112553</td>\n",
       "      <td>0</td>\n",
       "      <td>I really need help in this particular essay. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82755_1</td>\n",
       "      <td>82755</td>\n",
       "      <td>112560</td>\n",
       "      <td>1</td>\n",
       "      <td>Since this is an academic paper, please remove...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id  url_id  user_id  seqnum  \\\n",
       "0  82750_0   82750   112438       0   \n",
       "1  82750_1   82750    99412       1   \n",
       "2  82755_0   82755   112507       0   \n",
       "3  82784_0   82784   112553       0   \n",
       "4  82755_1   82755   112560       1   \n",
       "\n",
       "                                        post_content  \n",
       "0  Hi guys, I would need help reviewing this stud...  \n",
       "1  Yeu, I am afraid that this is not a valid stud...  \n",
       "2  Please confine your statement to no more than ...  \n",
       "3  I really need help in this particular essay. T...  \n",
       "4  Since this is an academic paper, please remove...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean html\n",
    "df_text['post_content'] = df_text['post_content'].str.replace('<br/>','\\n').apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "# assign unique id to each document\n",
    "df_text['doc_id'] = df_text['url_id'].astype(str) + '_' + df_text['seqnum'].astype(str)\n",
    "df_text = df_text[['doc_id','url_id','user_id','seqnum','post_content']]\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks for your advice!!! It does help!\\n\\nI\\'ll take your advice and adjust my essay. As you mentioned, \"ineffective sentence\" is quite a problem. I will try my best to revise my sentence in a concise, clear, and effective way.\\n\\nThanks again for your feedback!\\n\\n谢谢你😘'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text[df_text.post_content.str.contains(\"😘\")].post_content.values[0]#.replace(\"👍\",\":thumbsup:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['f', 'E', '_', ':', '|', 'I', '\"', 'A', '=', '6', '\\\\', ';', 'S',\n",
       "       'h', 'L', 'F', 'Y', '0', '?', '`', '(', 'x', 'B', '1', '2', 'k',\n",
       "       'p', 'w', '$', 'D', '<', 'o', ']', 'u', 'a', '&', 'C', 'U', 't',\n",
       "       'j', ',', '>', 'H', ')', \"'\", 'R', '{', 'J', '%', 'M', 'z', 'Z',\n",
       "       'v', '[', '~', '^', '9', '3', 'b', '8', 'W', 'X', '@', '}', 'i',\n",
       "       'm', '#', 'l', '+', 'G', 'N', '4', 'g', ' ', 'K', 'P', 'r', 'O',\n",
       "       'q', '!', '.', '7', '-', 'n', '/', '*', 'Q', 'c', 's', 'V', 'd',\n",
       "       'T', 'e', '5', '\\n', 'y'], dtype='<U1')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unq_chars = np.array([*set(\" \".join(df_text.post_content))]); unq_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text['post_content'] = df_text.post_content.apply(lambda x: unidecode.unidecode(x.replace(\"👍\",\" :thumbsup: \").replace(\"😘\",\" :kissing_heart: \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_piv = df_text.pivot('url_id','seqnum','user_id')\n",
    "op_multi = user_piv[user_piv.apply(lambda x: x[0] in x[1:].values,axis=1)]\n",
    "op_ids = op_multi.loc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origpost = df_text.query('seqnum == 0')\n",
    "df_op_reply = df_text.query('url_id.isin(@op_multi.index) & user_id.isin(@op_ids) & seqnum != 0')\n",
    "df_critiques = df_text.drop(df_op_reply.index).query('seqnum > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = df_origpost[['url_id','doc_id','post_content']]\n",
    "dfc = df_critiques[['url_id','doc_id','post_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3    1.0\n",
       "Name: user_likes, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sop_data['doc_id'] = sop_data.url_id.astype(str) + '_' + sop_data.seqnum.astype(str)\n",
    "\n",
    "df_users = sop_data.iloc[:, np.r_[-7:0, 3]]\n",
    "dfu_o = df_users.query('seqnum==0')\n",
    "dfu_c = df_users[df_users.doc_id.isin(df_critiques.doc_id)]\n",
    "dfu_oc = dfu_o.merge(dfu_c, on='url_id')\n",
    "\n",
    "dfu_c.user_likes.quantile([.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2574"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu_oc.query('user_likes_y > 0').url_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3107"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu_oc.url_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id_x</th>\n",
       "      <th>user_likes_x</th>\n",
       "      <th>user_posts_x</th>\n",
       "      <th>user_threads_x</th>\n",
       "      <th>username_x</th>\n",
       "      <th>doc_id_x</th>\n",
       "      <th>seqnum_x</th>\n",
       "      <th>user_id_y</th>\n",
       "      <th>user_likes_y</th>\n",
       "      <th>user_posts_y</th>\n",
       "      <th>user_threads_y</th>\n",
       "      <th>username_y</th>\n",
       "      <th>doc_id_y</th>\n",
       "      <th>seqnum_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82750</td>\n",
       "      <td>112438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>decimo1993</td>\n",
       "      <td>82750_0</td>\n",
       "      <td>0</td>\n",
       "      <td>99412</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>7529</td>\n",
       "      <td>0</td>\n",
       "      <td>Holt</td>\n",
       "      <td>82750_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82755</td>\n",
       "      <td>112507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Airydisc</td>\n",
       "      <td>82755_0</td>\n",
       "      <td>0</td>\n",
       "      <td>112560</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Constance</td>\n",
       "      <td>82755_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82784</td>\n",
       "      <td>112553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>itha20</td>\n",
       "      <td>82784_0</td>\n",
       "      <td>0</td>\n",
       "      <td>112560</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Constance</td>\n",
       "      <td>82784_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82784</td>\n",
       "      <td>112553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>itha20</td>\n",
       "      <td>82784_0</td>\n",
       "      <td>0</td>\n",
       "      <td>112562</td>\n",
       "      <td>241.0</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>Maria</td>\n",
       "      <td>82784_2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82784</td>\n",
       "      <td>112553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>itha20</td>\n",
       "      <td>82784_0</td>\n",
       "      <td>0</td>\n",
       "      <td>112095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>IamIana</td>\n",
       "      <td>82784_3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_id  user_id_x  user_likes_x  user_posts_x  user_threads_x  username_x  \\\n",
       "0   82750     112438           0.0             0               1  decimo1993   \n",
       "1   82755     112507           0.0             0               1    Airydisc   \n",
       "2   82784     112553           0.0             0               1      itha20   \n",
       "3   82784     112553           0.0             0               1      itha20   \n",
       "4   82784     112553           0.0             0               1      itha20   \n",
       "\n",
       "  doc_id_x  seqnum_x  user_id_y  user_likes_y  user_posts_y  user_threads_y  \\\n",
       "0  82750_0         0      99412        2001.0          7529               0   \n",
       "1  82755_0         0     112560           9.0            19               0   \n",
       "2  82784_0         0     112560           9.0            19               0   \n",
       "3  82784_0         0     112562         241.0           602               0   \n",
       "4  82784_0         0     112095           0.0             2               1   \n",
       "\n",
       "  username_y doc_id_y  seqnum_y  \n",
       "0       Holt  82750_1         1  \n",
       "1  Constance  82755_1         1  \n",
       "2  Constance  82784_1         1  \n",
       "3      Maria  82784_2         2  \n",
       "4    IamIana  82784_3         3  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu_oc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_likes</th>\n",
       "      <th>user_posts</th>\n",
       "      <th>user_threads</th>\n",
       "      <th>username</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>post_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82750</td>\n",
       "      <td>99412</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>7529</td>\n",
       "      <td>0</td>\n",
       "      <td>Holt</td>\n",
       "      <td>82750_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Yeu, I am afraid that this is not a valid stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82755</td>\n",
       "      <td>112560</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Constance</td>\n",
       "      <td>82755_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Since this is an academic paper, please remove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82784</td>\n",
       "      <td>112560</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Constance</td>\n",
       "      <td>82784_1</td>\n",
       "      <td>1</td>\n",
       "      <td>This comments are just some suggestions that m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82784</td>\n",
       "      <td>112562</td>\n",
       "      <td>241.0</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>Maria</td>\n",
       "      <td>82784_2</td>\n",
       "      <td>2</td>\n",
       "      <td>Itha20,\\n\\nLooking through your essay, I think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82784</td>\n",
       "      <td>112095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>IamIana</td>\n",
       "      <td>82784_3</td>\n",
       "      <td>3</td>\n",
       "      <td>I think your essay is a little messy. I would ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_id  user_id  user_likes  user_posts  user_threads   username   doc_id  \\\n",
       "0   82750    99412      2001.0        7529             0       Holt  82750_1   \n",
       "1   82755   112560         9.0          19             0  Constance  82755_1   \n",
       "2   82784   112560         9.0          19             0  Constance  82784_1   \n",
       "3   82784   112562       241.0         602             0      Maria  82784_2   \n",
       "4   82784   112095         0.0           2             1    IamIana  82784_3   \n",
       "\n",
       "   seqnum                                       post_content  \n",
       "0       1  Yeu, I am afraid that this is not a valid stud...  \n",
       "1       1  Since this is an academic paper, please remove...  \n",
       "2       1  This comments are just some suggestions that m...  \n",
       "3       2  Itha20,\\n\\nLooking through your essay, I think...  \n",
       "4       3  I think your essay is a little messy. I would ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu_c.merge(df_critiques).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu_c.merge(df_critiques).user_posts.quantile(.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "dfo.merge(dfc, on='url_id').to_pickle('proc_data/post_pair_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pd = Path('proc_data')\n",
    "path_al = path_pd/'all_texts'\n",
    "path_cr = path_pd/'critiques'\n",
    "path_op = path_pd/'op_replies'\n",
    "path_og = path_pd/'orig_posts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file_corpus(df, datapath, idcol='doc_id', postcol='post_content'):\n",
    "    print(f'Writting {df.shape[0]} files to: {datapath} ...')\n",
    "    for doc_id, content in df[[idcol,postcol]].values:\n",
    "        Path(datapath/doc_id).with_suffix('.txt').write_text(content)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_file_corpus(df_text, path_al)\n",
    "to_file_corpus(df_critiques, path_cr)\n",
    "to_file_corpus(df_op_reply, path_op)\n",
    "to_file_corpus(df_origpost, path_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3 - Strip Quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section was done with the intention of improving phrase extraction by removing quoted text from replies.\n",
    "\n",
    "Various references used:\n",
    "\n",
    "https://stackoverflow.com/questions/30285706/detecting-similar-paragraphs-in-two-documents\n",
    "\n",
    "https://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf\n",
    "\n",
    "https://en.wikipedia.org/wiki/W-shingling\n",
    "\n",
    "https://stackoverflow.com/questions/17740833/checking-fuzzy-approximate-substring-existing-in-a-longer-string-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "from unidecode import unidecode\n",
    "\n",
    "import Levenshtein\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** compare similarity between every sentence in original post with review posts. Determine a suitable threshold and strip out those sentences from reviews.\n",
    "\n",
    "Quotes are unreliable for this task because people often quote small phrases. There is a potential for stripping out long quote surrounded passages, but this is likely still going to be less effective.\n",
    "\n",
    "[Key Phrase Extraction](https://en.wikipedia.org/wiki/Automatic_summarization#Keyphrase_extraction) is less effective when there are large quoted passages. Stripping these out will almost certainly improve the extraction process and will likely help with summarization as well.\n",
    "\n",
    "[Sent2Vec](https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/) may or may not be useful in this task. Something as simple as a BLEU score could be equally, or even more, effective.\n",
    "\n",
    "The ideal metric would disregard the length of sentences, or at least discount them heavily. Weight should be given to number of matching consecutive terms where order is important. However, this also opens several problems. Reviewers have a tendency to inject words, strip words, and ellipsize passages.\n",
    "\n",
    "Examples:\n",
    "\n",
    "should have high similarity\n",
    "```\n",
    "OP: I landed a job with TCS, Asia's largest IT service provider.\n",
    "REV: I landed a --job-- position with TCS, Asia's largest ... provider.\n",
    "```\n",
    "\n",
    "should have low similarity\n",
    "```\n",
    "OP: I have been given many suggestions throughout my career\n",
    "REV: I have many suggestions for improvements throughout your statement\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "#nlp.disable_pipes('tagger','ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_pickle('proc_data/minparse/full_text_minproc.pkl')\n",
    "op_idx = np.load('proc_data/minparse/op_idx.npy')\n",
    "op_rep_idx = np.load('proc_data/minparse/op_rep_idx.npy')\n",
    "crit_idx = np.load('proc_data/minparse/crit_idx.npy')\n",
    "\n",
    "df_op = df_text.loc[op_idx]\n",
    "df_crit = df_text.loc[crit_idx]\n",
    "\n",
    "df_op = df_op.drop(['user_id','seqnum','tokens'],1)\n",
    "df_crit = df_crit.drop(['user_id','seqnum','tokens'],1)\n",
    "\n",
    "df_merged = df_op.merge(df_crit, on='url_id', suffixes=('_op','_rv'))[['doc_id_rv','post_content_op','post_content_rv']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple base cases, suggestions that contain the word \"change\", and suggestions that have a quotation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id_rv</th>\n",
       "      <th>post_content_op</th>\n",
       "      <th>post_content_rv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82784_1</td>\n",
       "      <td>I really need help in this particular essay. T...</td>\n",
       "      <td>This comments are just some suggestions that m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>79311_1</td>\n",
       "      <td>Question 2: The Ivey MSc. Business Analytics r...</td>\n",
       "      <td>Funmi, your response to the prompt is too acad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>78374_2</td>\n",
       "      <td>learning journey in humanitarian and developme...</td>\n",
       "      <td>Asli, this is not exactly a proper statement o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>79386_1</td>\n",
       "      <td>application essay to the financial sector\\n\\nH...</td>\n",
       "      <td>Xue, your discussion is misdirected. The more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>75532_1</td>\n",
       "      <td>Hello guys\\nHere is my essay, please tell me w...</td>\n",
       "      <td>Robin, you have too much going on in your firs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id_rv                                    post_content_op  \\\n",
       "2    82784_1  I really need help in this particular essay. T...   \n",
       "8    79311_1  Question 2: The Ivey MSc. Business Analytics r...   \n",
       "11   78374_2  learning journey in humanitarian and developme...   \n",
       "15   79386_1  application essay to the financial sector\\n\\nH...   \n",
       "31   75532_1  Hello guys\\nHere is my essay, please tell me w...   \n",
       "\n",
       "                                      post_content_rv  \n",
       "2   This comments are just some suggestions that m...  \n",
       "8   Funmi, your response to the prompt is too acad...  \n",
       "11  Asli, this is not exactly a proper statement o...  \n",
       "15  Xue, your discussion is misdirected. The more ...  \n",
       "31  Robin, you have too much going on in your firs...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged[df_merged.post_content_rv.str.contains('change',case=False)&~df_merged.post_content_op.str.contains('change',case=False)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "quodf = df_merged[df_merged.post_content_rv.str.contains('\"')]\n",
    "quoted_texts = quodf.post_content_rv.str.findall(r'(\"[^\"]*\")') # store quoted strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip out quoted texts from reviews\n",
    "df_merged.loc[quodf.index, 'post_content_rv'] = quodf.post_content_rv.str.replace(r'(\"[^\"]*\")','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this approach is by no means foolproof, but it's a start and we shouldn't loose too much information of value since the goal is filter down to editing terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99      ['Internet of Things (IoT)', '&', 'and', 'a St...\n",
       "165                                                 ['d']\n",
       "274     ['I', 'I specifically want to...', 'Solving ch...\n",
       "286                                                ['Hi']\n",
       "291             ['but', 'and', 'However', 'Nevertheless']\n",
       "                              ...                        \n",
       "5440                                  ['but', 'although']\n",
       "5468    ['s business successful. Personally, I believe...\n",
       "5469                   ['curiosity about the balance...']\n",
       "5515                                            ['ímply']\n",
       "5524                                    ['strategically']\n",
       "Name: post_content_rv, Length: 218, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged[df_merged.post_content_rv.str.findall(r\"(\\B'[^']+')\").apply(len) > 0].post_content_rv.str.findall(r\"(\\B'[^']+')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single quotes are a bit tricker, semantically, they can have several different meanings which may or may not be quoted text from the author, not to mention their use as apostrophes. For these reasons, we will leave them in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_pipe(s):\n",
    "    return gensim.parsing.strip_multiple_whitespaces(gensim.parsing.strip_non_alphanum(unidecode(s))).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vord = np.vectorize(ord,otypes=['uint16'])\n",
    "def fingerprint(s,ngram=3,tonum=False):\n",
    "    s_fp = strip_pipe(s).replace(' ','')\n",
    "    chargrams = np.array([*nltk.ngrams(s_fp,ngram)])\n",
    "    return vord(chargrams) if tonum else chargrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df_merged.iloc[:,1:].sample(1).iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi1i1b1bebegegigininnnnininingngogofofefeaeacachchshsesenentntetenencnceceseshshohououluldldbdbebeieininanacacacapapipititatalalllleletetttteterer2r2p2plpleleaeasasesegegigivivevebeblblalananknkskspspapacaceceaeafaftftetererereaeacachchchcocomommmmamaqaququeueseststitioiononmnmamararkrkakanandndcdcocomompmplpleletetitioionononofofsfsesenentntetenencnceceseststrtryrytytotoaoavavovoioididtdththehesesesesesmsmamalallllmlmimisiststatakakekesesasanandndtdtrtryrytytotoeoexexpxplplalaiaininynyoyouoururirididedeaeasasisinindndedetetataiaililslsusucuchchahasaswswhwhahatatitisiststeteaeacachchfhfofororirinindndidiaiaiaisistsththihisisasananynymymomovovevemememenentntotofofafacacacadadedememymywywhwhahatatdtdodotoththeheyeywywowororkrksksosotoththahatatrtrereaeadadederercrcacananununundndederersrststatanandndjdjujusuststatasasusuguggggegeseststitioiononbnbebeteththehecechchahanangngegeyeyoyououwuwawanantnttttotososeseeeeieinintnththehewewowororlrldldldlilininenesesbsbybygygaganandndhdhihiiiininsnspspipirireresesmsmemeaeanandndedededuducucacatatitioiononinisisosononeneaeararereaeawawhwhihicichchwhwiwilillllllleleaeadadtdththehecechchahanangngegeieinintnththehececocomomiminingngcgcecenentntutururyrybybubututdtdodoeoesesmsmamajajojororirititytyiyinindndidiaiananpnpepeoeopoplplelehehahavaveveaeacaccccecesesssststotoqoququaualalilititytyeyededuducucacatatitioionontnththeheaeanansnswswewereririsisnsnonoioiiiiaialalwlwawayaysyscscacapapipititatalalwlwawanantnttttotocochchahanangngegeteththihisisasanandndbdbebelelilieieveveveieinintnththehepeprpririnincncicipiplpleleoeofoftftrtrereeeeaeatatrtrereeeeseststatarartrtstswswiwitiththahasaseseeeededadanandndtdththehenensnsgsgrgrorowowswsasanandndbdbebeaeararsrsfsfrfruruiuititstsasanandndsdseseeeededsdswswhwhihilileleleleleaeadadtdtotoaoafafofororereseststitinininitititiaialallllylyiyiwiwawanantnttttotololeleaeararnrnanababobouoututttththehecechchahalallllelenengngegesesfsfafacacecededbdbybytyththeheeeededuducucacatatitioiononsnsesecectctotororirininenededuducucacatatitiningngmgmamasassssesesespsprpraracactctiticicacalallllylywywowororkrkikiningngwgwiwitithththteteaeacachchfhfofororirinindndidiaiawawhwhihicichchwhwiwilillllhlhehelelplpipininlnleleaeararnrnininingngtgththehebebabasasisicicscsasanandndadalalslsosomomamasaststetereririningngtgththehememomovovevererarapapepereririoiodododofoftftwtwowoyoyeyeaeararsrsisititwtwiwilillllblbebeseststetepeppppipiningngsgststotononenetetotowowawarardrdsdsmsmymyjyjojouoururnrneneyeyfyfofororlrleleaeadadidiningngiginindndidiaiaiainindndidiaiasashshohououluldldbdbebeieinincncacapapipititatalaltltotoaoaeaededuducucacatatitioiononanalalrlrerefefoforormrmwmwhwhahatatstsbsbebetetttteterertrththahananjnjojoioiningngtgteteaeacachchfhfofororirinindndidiaiawawhwhihicichchhhhahasasmsmamadadedeaeararerememamararkrkakabablblelememamararkrkwkwiwitiththihininfnfefewewywyeyeaeararsrsosofofifititstsjsjojouoururnrneneyeyhyhehererereteththeherererewewiwilillllplpepeoeopoplplelefefrfroromomwmwhwhohomomimicicacananlnleleaeararnrnananandndidimimpmprprorovovevetetatakakekeaeadadvdvivicicecesesasanandndpdpupututftfofororwrwawarardrdmdmymyoyopopipinininioiononsnsisininanababebetettttetererwrwawayaytyththahanantnththehepeprpraracactctiticicacalalklknknonowowlwleledededegegegewewhwhihicichchihisisisimimamapaprprtrtetededidinintnththehebebsbscschchohoooololslsisiwiwawanantnttttotodododoaoananmnmbmbabaiaininrnrurururaralalmlmamanananagagegemememenentntftfrfroromomimirirmrmamaaaananananandndadanandndsdststatarartrtatananenededuducucacatatitioiononanalalmlmomovovevemememenentntitininrnrurururaralalilinindndidiaiawawhwhehererereqeququaualalilititytyeyededuducucacatatitioiononinisisssststitilillllaladadrdrereaeamamfmfoforormrmamananynyhyhihisiststotororiricicacalallllylymymamananynyoyofoftfththehegegrgrereaeatatetesestststscscicieienentntitieieseststatanandndidininfnflflulununcncecerersrshshahavavevebebebeeeenenfnfrfroromomrmrurururaralalplpaparartrtstsosofofifinindndidiaiaiainindndidiaiaiaisisasagagrgrereaeatatctcocouoututrtryrywywiwitiththahapapopowowewererfrfufulullllrlreresesosouoururcrceceseststitilillllblbebeieiningngugununpnprprorococecesessssesededidififtfththihisisisisisssskskikilillllflfufulullllylydydedevevevelelolopopepededidinintntotoaoababoboooononananandndndnonototatabababananeneoeonontnththeherereresesosouoururcrcecesesisititwtwiwilillllolononenededadayaylyleleaeadadidinindndidiaiatatotomomumucuchchmhmomororerepeprprorododuducuctctitiviveveaeanandnddddedevevevelelolopopepededcdcocouoununtntrtryrywywhwhihilileleseststitilillllhlhohololdldidiningngigititstscscuculultltutururaralaldldidiviveverersrsisititytygygogoooododldlulucuckcktkththahananknksksasanana'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(fingerprint(samp[1]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 'the', 'change'),\n",
       " ('the', 'change', 'you'),\n",
       " ('change', 'you', 'want'),\n",
       " ('you', 'want', 'to'),\n",
       " ('want', 'to', 'see'),\n",
       " ('to', 'see', 'in'),\n",
       " ('see', 'in', 'the'),\n",
       " ('in', 'the', 'world'),\n",
       " ('the', 'world', 'lines'),\n",
       " ('world', 'lines', 'by')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*nltk.ngrams(strip_pipe(samp[0]).split(), 3)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bleusim(op_doc, rv_doc, minsim=0.20, strip_nalpn=True, smoothing=None):\n",
    "    \"\"\"Find similar sentences using Bleu scores\n",
    "    \n",
    "    Args:\n",
    "        op_doc, rv_doc : str, paragraphs to search and compare to similarity measures\n",
    "        minsim : float, minimum bleu score to consider a sentence pair a match\n",
    "        strip_nalpn: bool, if True will strip non-alphanumeric chars and consecutive whitespace\n",
    "        smoothing : a nltk.translate.bleu_score.SmoothingFunction method to correct for mismatch ngrams\n",
    "    \"\"\"\n",
    "    op_doc,rv_doc = (nlp(op_doc), nlp(rv_doc)) if isinstance(op_doc,str) else (op_doc,rv_doc)\n",
    "    \n",
    "    for sent_op,sent_rv in itertools.product(op_doc.sents,rv_doc.sents):\n",
    "        op_text, rv_text = strip_pipe(sent_op.text), strip_pipe(sent_rv.text)\n",
    "    \n",
    "        bleuscore = nltk.translate.bleu([op_text.split()], rv_text.split(), smoothing_function=smoothing)\n",
    "        brevpen = nltk.translate.bleu_score.brevity_penalty(len(op_text), len(rv_text))\n",
    "        if bleuscore > minsim:\n",
    "            print('OP: ', sent_op.text.strip())\n",
    "            print('REV:', sent_rv.text.strip())\n",
    "            print('BLEU: {:0.5f}, BrevityPenalty {:0.5f}, NoPenalty: {:0.5f}'.format(bleuscore, brevpen, bleuscore/brevpen))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_levsim(op_doc, rv_doc, minsim=0.01, strip_nalpn=True):\n",
    "    \"\"\"Find similar sentences using Levenshtein similarity\n",
    "    \n",
    "    Args:\n",
    "        op_doc, rv_doc : str, paragraphs to search and compare to similarity measures\n",
    "        minsim : float, minimum similarity to consider a sentence pair a match\n",
    "        strip_nalpn: bool, if True will strip non-alphanumeric chars and consecutive whitespace\n",
    "    \"\"\"\n",
    "    op_doc,rv_doc = (nlp(op_doc), nlp(rv_doc)) if isinstance(op_doc,str) else (op_doc,rv_doc)\n",
    "    for sent_op,sent_rv in itertools.product(op_doc.sents,rv_doc.sents):\n",
    "        op_text, rv_text = sent_op.text, sent_rv.text\n",
    "        if strip_nalpn:\n",
    "            op_text, rv_text = strip_pipe(sent_op.text), strip_pipe(sent_rv.text)\n",
    "            \n",
    "        sim = gensim.similarities.levenshtein.levsim(op_text, rv_text, min_similarity=minsim)\n",
    "        if sim > minsim:\n",
    "            print(f'SIM: {sim:0.6f}')\n",
    "            print('OP: ', sent_op.text.strip())\n",
    "            print('REV:', sent_rv.text.strip())\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzz_all(s1,s2):\n",
    "    \"\"\"Performs multiple fuzz similarity tests and returns array of calculations\"\"\"\n",
    "    pr = fuzz.partial_ratio(s1,s2)\n",
    "    ptsr = fuzz.partial_token_sort_ratio(s1,s2)\n",
    "    r = fuzz.ratio(s1,s2)\n",
    "    tsetr = fuzz.token_set_ratio(s1,s2)\n",
    "    tsortr = fuzz.token_sort_ratio(s1,s2)\n",
    "    qr = fuzz.QRatio(s1,s2)\n",
    "    ratios = np.array([pr,ptsr,r,tsetr,tsortr,qr])\n",
    "    return ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fuzzsim(op_doc, rv_doc, cos_sim_thresh=0.91, fzmean_thresh=50, fzmax_thresh=60, print_removals=True):\n",
    "    \"\"\"Find similar sentences using fuzzywuzzy and spaCy's cosine similarity\n",
    "    \n",
    "    Args:\n",
    "        op_doc, rv_doc : str, paragraphs to search and compare to similarity measures\n",
    "        cos_sim_thresh : float, minimum cosine similarity between sentences to consider a match\n",
    "        fzmean_thresh : int, minimum average fuzzy ratio between sentences\n",
    "        fzmax_thresh : int, minimum maximum fuzzy ratio value returned from `fuzz_all`\n",
    "        \n",
    "    yields:\n",
    "        str, matching sentences based on threshold criteria\n",
    "    \"\"\"\n",
    "    op_doc,rv_doc = (nlp(op_doc), nlp(rv_doc)) if isinstance(op_doc,str) else (op_doc,rv_doc)\n",
    "    \n",
    "    for sent_op,sent_rv in itertools.product(op_doc.sents,rv_doc.sents):\n",
    "        op_text, rv_text = map(gensim.parsing.strip_multiple_whitespaces, (sent_op.text, sent_rv.text))\n",
    "        \n",
    "        # check for empty vectors before calculating similarity\n",
    "        spcsim = sent_op.similarity(sent_rv) if sent_op.has_vector and sent_rv.has_vector else 0\n",
    "        \n",
    "        # first threshold check, significantly faster than checking with fuzz first\n",
    "        if spcsim > cos_sim_thresh:\n",
    "            ratios = fuzz_all(op_text, rv_text)\n",
    "            rmean,rmin,rmax = ratios.mean(),ratios.min(),ratios.max()\n",
    "            # second threshold to exclude content that is contextually similar but lexically different \n",
    "            if rmean > fzmean_thresh and rmax > fzmax_thresh:\n",
    "                if print_removals:\n",
    "                    print('OP: ', sent_op.text.strip())\n",
    "                    print('REV:', sent_rv.text.strip())\n",
    "                    print('Avg: {:0.2f}, Min: {}, Max: {}, CosSim: {:0.4f}'.format(rmean,rmin,rmax,spcsim))\n",
    "                    print('----------')\n",
    "                yield sent_rv.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_dup_sents(df, mincos=0.98):\n",
    "    \"\"\"Removes sentences from a dataframe, in place, that met the similarity threshold\"\"\"\n",
    "    for idx, (op_post, rev_post) in df.drop('doc_id_rv',1).iterrows():\n",
    "        for sent in find_fuzzsim(op_post,rev_post,cos_sim_thresh=mincos):\n",
    "            rev_post = rev_post.replace(sent,'')\n",
    "        df.loc[idx,'post_content_rv'] = rev_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop obvious mirrored sentences, i.e. cos sim > ~0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "rm_dup_sents(df_merged, mincos=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df_merged.to_feather('proc_data/merge_deduped.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_feather('proc_data/merge_deduped.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n",
      "rmsents: []\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "samp10 = df_merged.iloc[:,1:].sample(10).values\n",
    "for exop,exrv in samp10:\n",
    "    *rmsents, = find_fuzzsim(exop,exrv,cos_sim_thresh=0.98)\n",
    "    print('rmsents:',rmsents)\n",
    "    print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "================\n",
      "================\n",
      "================\n",
      "================\n",
      "OP:  It was on the Christmas Eve of 2010,\n",
      "REV: It was on the Christmas Eve of 2010, that night, I luckily found out a branch seizedthat piqued my curiosity\n",
      "BLEU: 0.34670, BrevityPenalty 1.00000, NoPenalty: 0.34670\n",
      "---------------\n",
      "OP:  that night, I luckily found out a branch seized my curiosity.\n",
      "REV: It was on the Christmas Eve of 2010, that night, I luckily found out a branch seizedthat piqued my curiosity\n",
      "BLEU: 0.37903, BrevityPenalty 1.00000, NoPenalty: 0.37903\n",
      "---------------\n",
      "================\n",
      "================\n",
      "================\n",
      "================\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "for exop,exrv in samp10:\n",
    "    find_bleusim(exop,exrv, minsim=0.25, smoothing=smoothing.method1)\n",
    "    print('================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our filtering method is not perfect, some sentences were sufficiently different to slip passed the threshold. Bleu score can help identify these misses, but choosing the correct threshold is particularly difficult. Additionally, this steps well outside the metrics intended use. It was developed for translation tasks and alternative use has quite a few [caveats to consider](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
